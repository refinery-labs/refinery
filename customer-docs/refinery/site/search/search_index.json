{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Refinery Docs This documentation explains all of Refinery's various options and features. If you have a question which is not found in this documentation please contact us at support@refinery.io and one of us will reach out to you as soon as possible! Click here to read to the Getting Started intro!","title":"Home"},{"location":"#welcome-to-the-refinery-docs","text":"This documentation explains all of Refinery's various options and features. If you have a question which is not found in this documentation please contact us at support@refinery.io and one of us will reach out to you as soon as possible! Click here to read to the Getting Started intro!","title":"Welcome to the Refinery Docs"},{"location":"architecture/","text":"Architecture Refinery's base-architecture is fairly simple. The only \"real\" additional infrastructure which is added into the equation is a single redis instance (or cluster) which is a core part of how Refinery achieves things like idempotency (ensuring your Lambdas execute only once), message passing, and other core work. Why redis? Redis is an extremely fast datastore which allows for quick retrieval, storage, and atomic data operations. While adding redis does make the infrastructure less \"serverless\" it's currently required to perform many of Refinery's features. Since redis is so fast it's likely not going to be a choke point for your deployments. However, if you have extremely high loads then you can also make use of an array of redis instances (simply contact us for more information) for extreme throughput. It is possible that in the future there will be enough concrete serverless functionality in AWS to remove the need for redis, but in the mean time we don't want to be contrained by it. Refinery Diagram The above diagram demonstrates the general layout of deployed Refinery projects in your infrastructure. All of your deployed Lambdas will use redis as a way to perform transitions and to perform other core functionality.","title":"Architecture"},{"location":"architecture/#architecture","text":"Refinery's base-architecture is fairly simple. The only \"real\" additional infrastructure which is added into the equation is a single redis instance (or cluster) which is a core part of how Refinery achieves things like idempotency (ensuring your Lambdas execute only once), message passing, and other core work.","title":"Architecture"},{"location":"architecture/#why-redis","text":"Redis is an extremely fast datastore which allows for quick retrieval, storage, and atomic data operations. While adding redis does make the infrastructure less \"serverless\" it's currently required to perform many of Refinery's features. Since redis is so fast it's likely not going to be a choke point for your deployments. However, if you have extremely high loads then you can also make use of an array of redis instances (simply contact us for more information) for extreme throughput. It is possible that in the future there will be enough concrete serverless functionality in AWS to remove the need for redis, but in the mean time we don't want to be contrained by it.","title":"Why redis?"},{"location":"architecture/#refinery-diagram","text":"The above diagram demonstrates the general layout of deployed Refinery projects in your infrastructure. All of your deployed Lambdas will use redis as a way to perform transitions and to perform other core functionality.","title":"Refinery Diagram"},{"location":"debugging/","text":"Debugging Logging Debugging in serverless environments can often be a complex chore which feels like working with a black box . Refinery attempts to greately simplify the debugging process through the use of its scalable logging system built on top of S3. The logging system in Refinery is configurable and allows for extreme verbosity when needed to help pinpoint and easily reproduce problems. What Refinery Logs Before we dive into debugging your service, we first have to talk about logging. Refinery logs a large amount of metadata about a Lambda's given execution. For example, some of the data which is logged is the following: The full input to the Lambda. The full returned output of the Lambda. All stdout and stderr outputted during execution. Time of execution. The reason for this level of verbosity is to allow easy reproduction of issues. Since development in Refinery is accomplished by building small functional blocks in a pipeline fashion, logging the full input and output allows developers to easy replay the same inputs into the same blocks in the IDE in order to replicate the issue. Configuring Your Logging Level Refinery allows for configuring different levels of logging for when your service is deployed in production. The levels of logging available are: Log all executions : Every time a Lambda is executed a log file is written. Log only errors : A log file is written only when a Lambda encounters an uncaught exception. No logging : No logs are written under any circumstance. The major tradeoff with logging all executions, error only logging, and no logging is cost. The Lambdas that Refinery deploys make use of S3 for storing of all of the created log objects. This means that you will incur the appropriate level of cost for each write you do to S3. As of the time of this writing, saving an object to S3 is billed at $0.005 per 1,000 requests . While this may seem inexpensive, it can add up if you're doing full verbosity logging on pipelines with a large number of executions. It's important to always be mindful about the amount of resources you're consuming when using Refinery. In order to configure your logging level for a given project, select Project Settings under the Project Options menu: The following dialogue will be presented which allows for changing the logging level: Note Changes in the logging level require a re-deploy to take effect. Viewing Logs of a Deployed Project After deploying a project with the logging level set to Log all executions or Log only errors you can then review the logs by clicking on View Production Deployment followed by clicking View Log(s) . The above screenshot demonstrates an example execution log group. In Refinery, logging is grouped by unique triggers of the deployed pipeline. What this means is that anytime a trigger (such as a Schedule Trigger , or an API Endpoint ) starts a pipeline's execution, a new execution ID will be generated. Multiple Lambda executions can be contained under a single pipeline execution ID. In the above screenshot you can see that the pipeline execution group is colored red. This is because somewhere in this pipeline's execution an uncaught exception occured. In Refinery if even a single Lambda in a pipeline's execution results in an uncaught exception being thrown the entire pipeline execution will be marked red. This is to ensure that errors do not go unmissed and to provide a clear distinction between a pipeline which executed completely without errors and a pipline which experienced no issues. Click on the pipeline execution box to view details about it. You'll see that upon viewing a specific pipeline execution that your deployment will have green and red boxes drawn over each Lambda: This works in a very similar way to how coloring works for the pipeline execution log groups. If a Lambda is executed three times and one of the executions results in an uncaught exception the box will be red. If none of the executions of a given Lambda resulted in an uncaught exception the box with be green. If the Lambda was never executed in that pipeline execution it will not be colored. To view more information about a Lambda's execution(s), simply click on the Lambda itself. In this case we'll view the 100x Lambda's executions to understand what caused the uncaught exception. Upon clicking on the Lambda we get the following: The above screenshot shows that the Lambda executed successfully five times and encountered an uncaught exception once. Since we want to understand what caused this exception we'll click on the red line to get more information about that specific execution: The above screenshot shows the details of the execution. We can see that the input to the Lambda was the integer 0 and the stack trace was the following: Traceback ( most recent call last ): File /var/task/lambda.py , line 1089 , in _init return_data = main ( lambda_input , context ) File /var/task/lambda.py , line 4 , in main return ( 100 / lambda_input ) ZeroDivisionError : integer division or modulo by zero The code, which you can probably infer from the stack trace, was the following: def main ( lambda_input , context ): print ( Dividing 100 by + str ( lambda_input ) + ... ) return ( 100 / lambda_input ) We can clearly see that the cause of this problem was the input to the Lambda being 0 which we divided the number 100 by. This caused an uncaught ZeroDivisionError exception which was logged by Refinery as a failure. While this particular issue was easy to spot, in more complex scenarios it may be harder to find the root cause of an error. In these situations you can better debug the problem by copying the input under the Input Data section and running it through the Lambda in the IDE. The following screenshot demonstrates this process: The above screenshot shows our simplistic example of passing in the 0 as input to this particular Lambda in the IDE. We get the same exception as we did in production and we can easily modify the code here to find the root cause without touching the production deployment. Once we have a fix we can simply deploy it to production to remediate the problem.","title":"Debugging & Logging"},{"location":"debugging/#debugging-logging","text":"Debugging in serverless environments can often be a complex chore which feels like working with a black box . Refinery attempts to greately simplify the debugging process through the use of its scalable logging system built on top of S3. The logging system in Refinery is configurable and allows for extreme verbosity when needed to help pinpoint and easily reproduce problems.","title":"Debugging &amp; Logging"},{"location":"debugging/#what-refinery-logs","text":"Before we dive into debugging your service, we first have to talk about logging. Refinery logs a large amount of metadata about a Lambda's given execution. For example, some of the data which is logged is the following: The full input to the Lambda. The full returned output of the Lambda. All stdout and stderr outputted during execution. Time of execution. The reason for this level of verbosity is to allow easy reproduction of issues. Since development in Refinery is accomplished by building small functional blocks in a pipeline fashion, logging the full input and output allows developers to easy replay the same inputs into the same blocks in the IDE in order to replicate the issue.","title":"What Refinery Logs"},{"location":"debugging/#configuring-your-logging-level","text":"Refinery allows for configuring different levels of logging for when your service is deployed in production. The levels of logging available are: Log all executions : Every time a Lambda is executed a log file is written. Log only errors : A log file is written only when a Lambda encounters an uncaught exception. No logging : No logs are written under any circumstance. The major tradeoff with logging all executions, error only logging, and no logging is cost. The Lambdas that Refinery deploys make use of S3 for storing of all of the created log objects. This means that you will incur the appropriate level of cost for each write you do to S3. As of the time of this writing, saving an object to S3 is billed at $0.005 per 1,000 requests . While this may seem inexpensive, it can add up if you're doing full verbosity logging on pipelines with a large number of executions. It's important to always be mindful about the amount of resources you're consuming when using Refinery. In order to configure your logging level for a given project, select Project Settings under the Project Options menu: The following dialogue will be presented which allows for changing the logging level: Note Changes in the logging level require a re-deploy to take effect.","title":"Configuring Your Logging Level"},{"location":"debugging/#viewing-logs-of-a-deployed-project","text":"After deploying a project with the logging level set to Log all executions or Log only errors you can then review the logs by clicking on View Production Deployment followed by clicking View Log(s) . The above screenshot demonstrates an example execution log group. In Refinery, logging is grouped by unique triggers of the deployed pipeline. What this means is that anytime a trigger (such as a Schedule Trigger , or an API Endpoint ) starts a pipeline's execution, a new execution ID will be generated. Multiple Lambda executions can be contained under a single pipeline execution ID. In the above screenshot you can see that the pipeline execution group is colored red. This is because somewhere in this pipeline's execution an uncaught exception occured. In Refinery if even a single Lambda in a pipeline's execution results in an uncaught exception being thrown the entire pipeline execution will be marked red. This is to ensure that errors do not go unmissed and to provide a clear distinction between a pipeline which executed completely without errors and a pipline which experienced no issues. Click on the pipeline execution box to view details about it. You'll see that upon viewing a specific pipeline execution that your deployment will have green and red boxes drawn over each Lambda: This works in a very similar way to how coloring works for the pipeline execution log groups. If a Lambda is executed three times and one of the executions results in an uncaught exception the box will be red. If none of the executions of a given Lambda resulted in an uncaught exception the box with be green. If the Lambda was never executed in that pipeline execution it will not be colored. To view more information about a Lambda's execution(s), simply click on the Lambda itself. In this case we'll view the 100x Lambda's executions to understand what caused the uncaught exception. Upon clicking on the Lambda we get the following: The above screenshot shows that the Lambda executed successfully five times and encountered an uncaught exception once. Since we want to understand what caused this exception we'll click on the red line to get more information about that specific execution: The above screenshot shows the details of the execution. We can see that the input to the Lambda was the integer 0 and the stack trace was the following: Traceback ( most recent call last ): File /var/task/lambda.py , line 1089 , in _init return_data = main ( lambda_input , context ) File /var/task/lambda.py , line 4 , in main return ( 100 / lambda_input ) ZeroDivisionError : integer division or modulo by zero The code, which you can probably infer from the stack trace, was the following: def main ( lambda_input , context ): print ( Dividing 100 by + str ( lambda_input ) + ... ) return ( 100 / lambda_input ) We can clearly see that the cause of this problem was the input to the Lambda being 0 which we divided the number 100 by. This caused an uncaught ZeroDivisionError exception which was logged by Refinery as a failure. While this particular issue was easy to spot, in more complex scenarios it may be harder to find the root cause of an error. In these situations you can better debug the problem by copying the input under the Input Data section and running it through the Lambda in the IDE. The following screenshot demonstrates this process: The above screenshot shows our simplistic example of passing in the 0 as input to this particular Lambda in the IDE. We get the same exception as we did in production and we can easily modify the code here to find the root cause without touching the production deployment. Once we have a fix we can simply deploy it to production to remediate the problem.","title":"Viewing Logs of a Deployed Project"},{"location":"nodes/","text":"Nodes Refinery supports a number of different node types, this documentation offers an easy reference for each one. Node Types Lambda Schedule Trigger SNS Topic Trigger SQS Queue Trigger API Endpoint API Response Lambda The Lambda node type is a node which executes code from one of Refinery's supported languages (currently only Python 2.7). This deploys to an AWS Lambda . Lambdas can take and return JSON-serializable data structures, and are subject to the limits described in the AWS Lambda Limits . These nodes make up the \"meat\" of most pipelines, performing small, functional operations as part of a bigger set of actions. Lambda's can also be saved and re-used easily, click the Save Lambda in Database button in the Lambda menu to save a given Lambda for later use. Settings Name : The name of the Lambda node. Language : The programming language for the Lambda, currently only Python 2.7 is supported. Import(s) : A list of libraries and packages used by the Lambda. For Python 2.7 Lambdas, any valid syntax for pip requirements files will work here. For more information on the format see the pip documentation . Code : This is the core code which will execute upon the Lambda being invoked. This must include the declaration of the function main which accepts two arguments lambda_input and context . lambda_input is the JSON-serializable input the Lambda was called with, and context contains metadata related to the Lambda's runtime. Execution Memory : This is the allocated memory for the Lambda to run with. The CPU power of the Lambda is scaled proportional to the amount of memory allocated. This factors into the costs of execution a given Lambda. Max Execution Time : This refers to the max amount of time a Lambda is allowed to execute for. Setting this to a low value allows prevention of \"zombie\" or \"runaway\" Lambdas from costing too much money with execution. Layers : The Lambda layers menu option allows for specifying the ARNs of Lambda layers for the Lambda to be deployed with. This allows for the addition of binaries, libraries, and even custom runtimes into a given Lambda. Environment Variables : This allows for specifying environment variables for the given Lambda. Environment variables allow for better seperation between configuration and code and can be changed in deployed pipelines without requiring a re-deploy. Options Update Lambda : Saves updates to the selected Lambda. This does not save the project, in order to save the project you must click the Save Project button. Environment Variables : Allows for changing the environment variables of a given Lambda. Delete Lambda : Deletes the selected Lambda along with any transitions to and from the node. Add Transition : Adds a transition to the Lambda, for more information on the different transition types see the Transitions section of this documentation. Run Lambda : Runs the Lambda with no specified input. Duplicate Lambda : Duplicates the currently selected Lambda and automatically renames the duplicate Lambda to not conflict with the original. Save Lambda in Database : Saves the Lambda to the Saved Lambdas database. This allows you to add it to other projects by selecting Saved Lambda from the Add New Resource dropdown. Schedule Trigger Executes the nodes which are linked to the Schedule Trigger node at a set interval. This can be something like every minute, every day at 5:00 PM, etc. Useful for operations which need to occur on a schedule in an a highly-reliable manner. This deploys as an AWS CloudWatch Event and can pass an optional JSON-serializable input to the connected nodes. Settings Name : The name of the Schedule Trigger Schedule Expression : An expression which defines how often the trigger should fire the connected nodes. This follows the AWS CloudWatch expression formats which are described here. . Valid values include rate(1 minute) and cron(*/2 * * * ? *) . Description : A simple description of what the trigger is meant to do. This field is optional but useful for documentation. Input Data : JSON-serializable data which can be optionally passed as input to the connected nodes when the trigger fires. Options Update Schedule Trigger : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete Schedule Trigger : Deletes the selected node along with any transitions to and from the node. SNS Topic Trigger Execute nodes which are linked to it with the contents of the published SNS topic . SNS topics are useful for situations where you want to execute multiple Lambdas at the same time with the same input published to a given topic. This is often referred to as a \"pub-sub\" pattern . In contrast to the SQS Queue Trigger , for example, an SNS topic has no concept of retries, queueing or polling. Settings Name : The name of the SNS Topic Trigger Options Update SNS Topic Trigger : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete SNS Topic : Deletes the selected node along with any transitions to and from the node. SQS Queue Trigger Creates an AWS SQS Queue which can be linked to Lambda nodes in order to set these nodes as \"pollers\" of the given SQS queue . Currently only Standard SQS queues can be created as FIFO queues do not support Lambda as a trigger. Click here to read me about the difference between AWS standard and FIFO queues. . This node makes sense for situations where you want automatic retries on messages which result in an exception being thrown on the connected Lambda node. The number of concurrent Lambda executions will scale upwards until the maximum is reached to handle the queue load. This scaling behavior is described in the \"Understanding Scaling Behavior\" AWS documentation. . Warning It is important to know that having multiple Lambda nodes connected to a single SQS queue will likely not work the way you expect. Since AWS Lambdas operate in a \"polling\" fashion, the messages will not be split up or duplicated across multiple Lambdas connected to the queue. Instead messages will randomly flow into the connected Lambdas in no structured way. For a simple way to invoke multiple Lambdas with the same input, see the SNS Topic Trigger section. More information on SNS vs SQS can be found here. Settings Name : The name of the SQS Queue Trigger Batch Size : The number of messages to pass into the connect Lambdas as JSON-serializable input. This can be up to 10 total messages. Content-Based De-duplication : Options to de-duplicate messages placed onto the SQS queue based on their contents. The max de-duplication window is five minutes. Options Update SQS Queue : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete SQS Queue : Deletes the selected node along with any transitions to and from the node. API Endpoint The API Endpoint node represents a single RESTful HTTP endpoint. Upon hitting the generated HTTP endpoint the connected Lambda nodes will be triggered with the parameters and other HTTP request metadata passed as input. API endpoints are useful for situations such as building a REST API on top of serverless, creating pipelines triggered by webhooks , and more. It's important to note that at a minimum an API Endpoint must be connected to a Lambda node and the Lambda node (or some Lambda node in the pipeline) must transition into an API Response node. Warning As of this time, AWS has a hard limit of 29 seconds before timing out HTTP requests made to API Gateways (which are what API Endpoints deploy as). This complicates using API Endpoints for RESTful APIs because of the likelyhood of the computation taking longer than 29 seconds to finish (resulting in the API Gateway timing out). Settings Name : The name of the API Endpoint HTTP Method : The HTTP method for the API Endpoint to accept. Note that there can be multiple endpoints with the same path but with different HTTP methods. Path : The HTTP path. Options Update API Endpoint : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete API Endpoint : Deletes the selected node along with any transitions to and from the node. API Response API Response is a node which will return the data returned from a linked Lambda as an HTTP response. An API Response node is used downstream in a chain of Lambdas which started with an API Endpoint trigger. Note that due to the hard AWS limit of 29 seconds before API Endpoints time out, the transition to an API Response must occur in this time frame. If the pipeline execution starts with an API Endpoint and the intermediary node executions take longer than 29 seconds the request will time out. If the data passed as input to the API Response node does not contain the body key, then the data will be returned as a JSON blob in a HTTP response with the Content-Type set to application/json . For finer-grained control over the HTTP response, such as the ability to set headers, status codes, and more, return a JSON structure which complies with the proper format for 1AWS HTTP responses .","title":"Nodes"},{"location":"nodes/#nodes","text":"Refinery supports a number of different node types, this documentation offers an easy reference for each one.","title":"Nodes"},{"location":"nodes/#node-types","text":"Lambda Schedule Trigger SNS Topic Trigger SQS Queue Trigger API Endpoint API Response","title":"Node Types"},{"location":"nodes/#lambda","text":"The Lambda node type is a node which executes code from one of Refinery's supported languages (currently only Python 2.7). This deploys to an AWS Lambda . Lambdas can take and return JSON-serializable data structures, and are subject to the limits described in the AWS Lambda Limits . These nodes make up the \"meat\" of most pipelines, performing small, functional operations as part of a bigger set of actions. Lambda's can also be saved and re-used easily, click the Save Lambda in Database button in the Lambda menu to save a given Lambda for later use.","title":"Lambda"},{"location":"nodes/#settings","text":"Name : The name of the Lambda node. Language : The programming language for the Lambda, currently only Python 2.7 is supported. Import(s) : A list of libraries and packages used by the Lambda. For Python 2.7 Lambdas, any valid syntax for pip requirements files will work here. For more information on the format see the pip documentation . Code : This is the core code which will execute upon the Lambda being invoked. This must include the declaration of the function main which accepts two arguments lambda_input and context . lambda_input is the JSON-serializable input the Lambda was called with, and context contains metadata related to the Lambda's runtime. Execution Memory : This is the allocated memory for the Lambda to run with. The CPU power of the Lambda is scaled proportional to the amount of memory allocated. This factors into the costs of execution a given Lambda. Max Execution Time : This refers to the max amount of time a Lambda is allowed to execute for. Setting this to a low value allows prevention of \"zombie\" or \"runaway\" Lambdas from costing too much money with execution. Layers : The Lambda layers menu option allows for specifying the ARNs of Lambda layers for the Lambda to be deployed with. This allows for the addition of binaries, libraries, and even custom runtimes into a given Lambda. Environment Variables : This allows for specifying environment variables for the given Lambda. Environment variables allow for better seperation between configuration and code and can be changed in deployed pipelines without requiring a re-deploy.","title":"Settings"},{"location":"nodes/#options","text":"Update Lambda : Saves updates to the selected Lambda. This does not save the project, in order to save the project you must click the Save Project button. Environment Variables : Allows for changing the environment variables of a given Lambda. Delete Lambda : Deletes the selected Lambda along with any transitions to and from the node. Add Transition : Adds a transition to the Lambda, for more information on the different transition types see the Transitions section of this documentation. Run Lambda : Runs the Lambda with no specified input. Duplicate Lambda : Duplicates the currently selected Lambda and automatically renames the duplicate Lambda to not conflict with the original. Save Lambda in Database : Saves the Lambda to the Saved Lambdas database. This allows you to add it to other projects by selecting Saved Lambda from the Add New Resource dropdown.","title":"Options"},{"location":"nodes/#schedule-trigger","text":"Executes the nodes which are linked to the Schedule Trigger node at a set interval. This can be something like every minute, every day at 5:00 PM, etc. Useful for operations which need to occur on a schedule in an a highly-reliable manner. This deploys as an AWS CloudWatch Event and can pass an optional JSON-serializable input to the connected nodes.","title":"Schedule Trigger"},{"location":"nodes/#settings_1","text":"Name : The name of the Schedule Trigger Schedule Expression : An expression which defines how often the trigger should fire the connected nodes. This follows the AWS CloudWatch expression formats which are described here. . Valid values include rate(1 minute) and cron(*/2 * * * ? *) . Description : A simple description of what the trigger is meant to do. This field is optional but useful for documentation. Input Data : JSON-serializable data which can be optionally passed as input to the connected nodes when the trigger fires.","title":"Settings"},{"location":"nodes/#options_1","text":"Update Schedule Trigger : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete Schedule Trigger : Deletes the selected node along with any transitions to and from the node.","title":"Options"},{"location":"nodes/#sns-topic-trigger","text":"Execute nodes which are linked to it with the contents of the published SNS topic . SNS topics are useful for situations where you want to execute multiple Lambdas at the same time with the same input published to a given topic. This is often referred to as a \"pub-sub\" pattern . In contrast to the SQS Queue Trigger , for example, an SNS topic has no concept of retries, queueing or polling.","title":"SNS Topic Trigger"},{"location":"nodes/#settings_2","text":"Name : The name of the SNS Topic Trigger","title":"Settings"},{"location":"nodes/#options_2","text":"Update SNS Topic Trigger : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete SNS Topic : Deletes the selected node along with any transitions to and from the node.","title":"Options"},{"location":"nodes/#sqs-queue-trigger","text":"Creates an AWS SQS Queue which can be linked to Lambda nodes in order to set these nodes as \"pollers\" of the given SQS queue . Currently only Standard SQS queues can be created as FIFO queues do not support Lambda as a trigger. Click here to read me about the difference between AWS standard and FIFO queues. . This node makes sense for situations where you want automatic retries on messages which result in an exception being thrown on the connected Lambda node. The number of concurrent Lambda executions will scale upwards until the maximum is reached to handle the queue load. This scaling behavior is described in the \"Understanding Scaling Behavior\" AWS documentation. . Warning It is important to know that having multiple Lambda nodes connected to a single SQS queue will likely not work the way you expect. Since AWS Lambdas operate in a \"polling\" fashion, the messages will not be split up or duplicated across multiple Lambdas connected to the queue. Instead messages will randomly flow into the connected Lambdas in no structured way. For a simple way to invoke multiple Lambdas with the same input, see the SNS Topic Trigger section. More information on SNS vs SQS can be found here.","title":"SQS Queue Trigger"},{"location":"nodes/#settings_3","text":"Name : The name of the SQS Queue Trigger Batch Size : The number of messages to pass into the connect Lambdas as JSON-serializable input. This can be up to 10 total messages. Content-Based De-duplication : Options to de-duplicate messages placed onto the SQS queue based on their contents. The max de-duplication window is five minutes.","title":"Settings"},{"location":"nodes/#options_3","text":"Update SQS Queue : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete SQS Queue : Deletes the selected node along with any transitions to and from the node.","title":"Options"},{"location":"nodes/#api-endpoint","text":"The API Endpoint node represents a single RESTful HTTP endpoint. Upon hitting the generated HTTP endpoint the connected Lambda nodes will be triggered with the parameters and other HTTP request metadata passed as input. API endpoints are useful for situations such as building a REST API on top of serverless, creating pipelines triggered by webhooks , and more. It's important to note that at a minimum an API Endpoint must be connected to a Lambda node and the Lambda node (or some Lambda node in the pipeline) must transition into an API Response node. Warning As of this time, AWS has a hard limit of 29 seconds before timing out HTTP requests made to API Gateways (which are what API Endpoints deploy as). This complicates using API Endpoints for RESTful APIs because of the likelyhood of the computation taking longer than 29 seconds to finish (resulting in the API Gateway timing out).","title":"API Endpoint"},{"location":"nodes/#settings_4","text":"Name : The name of the API Endpoint HTTP Method : The HTTP method for the API Endpoint to accept. Note that there can be multiple endpoints with the same path but with different HTTP methods. Path : The HTTP path.","title":"Settings"},{"location":"nodes/#options_4","text":"Update API Endpoint : Updates the selected node with the updated settings. This does not save the project, in order to save the project you must click the Save Project button. Add Transition : Adds a transition to the node, for more information on the different transition types see the Transitions section of this documentation. Delete API Endpoint : Deletes the selected node along with any transitions to and from the node.","title":"Options"},{"location":"nodes/#api-response","text":"API Response is a node which will return the data returned from a linked Lambda as an HTTP response. An API Response node is used downstream in a chain of Lambdas which started with an API Endpoint trigger. Note that due to the hard AWS limit of 29 seconds before API Endpoints time out, the transition to an API Response must occur in this time frame. If the pipeline execution starts with an API Endpoint and the intermediary node executions take longer than 29 seconds the request will time out. If the data passed as input to the API Response node does not contain the body key, then the data will be returned as a JSON blob in a HTTP response with the Content-Type set to application/json . For finer-grained control over the HTTP response, such as the ability to set headers, status codes, and more, return a JSON structure which complies with the proper format for 1AWS HTTP responses .","title":"API Response"},{"location":"quickstart/intro/","text":"Getting Started This intro will walk you through creating a new Refinery project and deploying it to production. Creating a Project First navigate to your version of Refinery. You will be greeted with an empty canvas along with a greeting of Welcome to Refinery! . In the top-left corner you'll see the text New Project . This is the default name of a project in Refinery. Click on this red text to change the name of your current project: Upon clicking this text you'll see the follow dialogue: Here you can change your project's name to whatever you'd prefer. The only restriction is that project names must be unique and Refinery won't let you use a name which is in use for another project. For this demo, just set the name to Example Project (or whatever you'd prefer). Keep the version at 1 and click the Close button to close the dialogue. Saving the Project To save the project, click on the Project Options button and select Save Project to Current Version : Once you've done this you've now created your first (empty) project! You'll notice that the Save Project and Deploy to Production buttons have now appeared in the top-right corner of the page. These are for easy access to these actions while you develop your serverless applications. Warning It's important to ensure that you always save your progress. If you leave the page without saving your project you can loss all of the changes you've made. Nothing is more frusterating than lost work so ensure that you save your progress often. Additionally, if you make changes you wish you hadn't you can just refresh the page and re-open your project without saving to revert your change(s). Adding your first Lambda An empty project isn't useful for much, let's add something to the canvas! Start by clicking on the Add New Resource dropdown menu and selecting Lambda : This will add a Lambda named New Lambda to your canvas. Click on the Lambda icon to get options for editing it. You can now see some of the options you have for editing the Lambda. We'll start by renaming it to something new. In the Name field change New Lambda to Get Page and then scroll down the menu to the Update Lambda button. Click the button to save the new name to the Lambda. It's important to note that saving the Lambda does not save the project, these are explicitly different things. You will need to make sure to save the changes to the nodes you're working on as well as saving the entire project when you want to persist the changes. Now we will add the \"meat\" to our Lambda. We're going to make this Lambda grab a webpage and print the results. We'll start by adding the Python requests library to our Import(s) for our Lambda. Just add the text requests to the Import(s) text box similar to what is shown in the following screenshot: The items listed under Import(s) refer to Python pip packages. You can do just the regular package name such as requests or you can do a specific version like requests==2.21.0 . For the Python 2.7 language, any valid syntax for pip requirements files will work here. For more information on the format see the pip documentation . Now we'll add code to our Lambda, since it's hard to code in a tiny screen click the View Fullscreen button under the Code section of the Lambda menu. The following is the example code we'll be adding: import requests def main ( lambda_input , context ): print ( Grabbing webpage... ) response = requests . get ( https://www.google.com/robots.txt ) print ( ...done! ) return { html : response . text , status : response . status_code , } Feel free to just paste the above code in. Now, in the fullscreen editor click the Run Lambda button to try running the code. Once you do you'll see the following: Here we can see the full Lambda output, (ignore the START , END , and REPORT lines for now, as these are part of Lambda's internals). We can see the lines that we've printed along with the JSON that we returned. This brings us to an important point which is that in Refinery you always need to return data in a JSON-serializable format. This means that you can return things like strings, integers, and arrays, but full \"complex\" objects cannot be returned. Now that we've added some code let's save our progress by clicking the Save Lambda Project button. This will both save our changes to the Lambda and save our entire project. Now we can close our fullscreen editor by clicking outside of the box, or by clicking the Close button. Deploying the project We'll now actually deploy our project to production. Unlike with many conventional build systems, building and deploying to production doesn't take very long in Refinery. Note Refinery makes heavy use of dependency caching. The first time you add or change the Import(s) that you Lambda uses you will notice that deploying and running your Lambdas will take slightly longer. This is because Refinery builds Lambda \"templates\" out of these libraries and then injects your code into the cached templates during deployment/running. Once the cache has been built all future runs and deploys will take seconds. So entire deploys may take a minute or two the first time, but the second time they will take only a few seconds. Click the Deploy to Production button in the top right corner of the page to deploy your project. This should only take a few seconds, afterwards you'll be presented with the following: Click on the View Deployment Information button, this will take you to the Deployment Viewer. The deployment viewer allows you to see your deployed project's state and allows you to do things like view logs, debug pipelines, and see your deployment in the AWS console. Click on the Lambda you just created to see more information about it, as well as the options you can do with it. For now we're just going to run this Lambda while it's deployed. Click on the Run Lambda button to get a screen similar to the previous code editor. This is mostly the same, but you're not able to edit the code since it's currently deployed. Click Run Lambda to again run our created Lambda, you'll see the very familiar screen showing the output and the return value of the Lambda. Of course, just creating single Lambdas without any chaining doesn't show off Refinery's real power which lies in it's transitions and triggers. To show some of this off we're going to make this an API endpoint without writing any additional code! Painless API Endpoints Click the Tear Down Deployment button to tear down our deployed project and bring us back to the regular editor. Note Tearing down deployments in Refinery is always a very fast operation. Unlike conventional build systems and production infrastructure, there's no servers to turn down or terminate so deleting things is easy. Under the Add New Resources dropdown menu, select API Endpoint . Do so again, this time selecting API Response to add an API Response node to the canvas. We now have a bunch of nodes which we can link together. Click on the API Endpoint node and click the Add Transition button. You will now see a menu similar to the following: These default options are fine, we're adding a transition from the API Endpoint to the Lambda we created. Click the Add State Transition button to add this transition. Now, click on the Get Page Lambda node, and select Add Transition once again and under Next State , select API Response and click Add State Transition . You should now have a pipeline similar to the following: We've now done all we need to do to make this Lambda an API endpoint! Click the Deploy to Production button to deploy it. Now click View Deployment Information to see the newly-deployed project. To view your newly created API Endpoint , click the API Endpoint node and select the Deployed Endpoint URL . This link will open a new tab with your API Endpoint URL and the returned JSON response is simply what was returned from the Lambda: { status : 200 , html : User-agent : *\\nDisallow : /search\\nAllow : ...trimmed for brevity... Going Forward You've now made a simple project in Refinery, check out other parts of this documentation to create more complex pipelines!","title":"Getting Started"},{"location":"quickstart/intro/#getting-started","text":"This intro will walk you through creating a new Refinery project and deploying it to production.","title":"Getting Started"},{"location":"quickstart/intro/#creating-a-project","text":"First navigate to your version of Refinery. You will be greeted with an empty canvas along with a greeting of Welcome to Refinery! . In the top-left corner you'll see the text New Project . This is the default name of a project in Refinery. Click on this red text to change the name of your current project: Upon clicking this text you'll see the follow dialogue: Here you can change your project's name to whatever you'd prefer. The only restriction is that project names must be unique and Refinery won't let you use a name which is in use for another project. For this demo, just set the name to Example Project (or whatever you'd prefer). Keep the version at 1 and click the Close button to close the dialogue.","title":"Creating a Project"},{"location":"quickstart/intro/#saving-the-project","text":"To save the project, click on the Project Options button and select Save Project to Current Version : Once you've done this you've now created your first (empty) project! You'll notice that the Save Project and Deploy to Production buttons have now appeared in the top-right corner of the page. These are for easy access to these actions while you develop your serverless applications. Warning It's important to ensure that you always save your progress. If you leave the page without saving your project you can loss all of the changes you've made. Nothing is more frusterating than lost work so ensure that you save your progress often. Additionally, if you make changes you wish you hadn't you can just refresh the page and re-open your project without saving to revert your change(s).","title":"Saving the Project"},{"location":"quickstart/intro/#adding-your-first-lambda","text":"An empty project isn't useful for much, let's add something to the canvas! Start by clicking on the Add New Resource dropdown menu and selecting Lambda : This will add a Lambda named New Lambda to your canvas. Click on the Lambda icon to get options for editing it. You can now see some of the options you have for editing the Lambda. We'll start by renaming it to something new. In the Name field change New Lambda to Get Page and then scroll down the menu to the Update Lambda button. Click the button to save the new name to the Lambda. It's important to note that saving the Lambda does not save the project, these are explicitly different things. You will need to make sure to save the changes to the nodes you're working on as well as saving the entire project when you want to persist the changes. Now we will add the \"meat\" to our Lambda. We're going to make this Lambda grab a webpage and print the results. We'll start by adding the Python requests library to our Import(s) for our Lambda. Just add the text requests to the Import(s) text box similar to what is shown in the following screenshot: The items listed under Import(s) refer to Python pip packages. You can do just the regular package name such as requests or you can do a specific version like requests==2.21.0 . For the Python 2.7 language, any valid syntax for pip requirements files will work here. For more information on the format see the pip documentation . Now we'll add code to our Lambda, since it's hard to code in a tiny screen click the View Fullscreen button under the Code section of the Lambda menu. The following is the example code we'll be adding: import requests def main ( lambda_input , context ): print ( Grabbing webpage... ) response = requests . get ( https://www.google.com/robots.txt ) print ( ...done! ) return { html : response . text , status : response . status_code , } Feel free to just paste the above code in. Now, in the fullscreen editor click the Run Lambda button to try running the code. Once you do you'll see the following: Here we can see the full Lambda output, (ignore the START , END , and REPORT lines for now, as these are part of Lambda's internals). We can see the lines that we've printed along with the JSON that we returned. This brings us to an important point which is that in Refinery you always need to return data in a JSON-serializable format. This means that you can return things like strings, integers, and arrays, but full \"complex\" objects cannot be returned. Now that we've added some code let's save our progress by clicking the Save Lambda Project button. This will both save our changes to the Lambda and save our entire project. Now we can close our fullscreen editor by clicking outside of the box, or by clicking the Close button.","title":"Adding your first Lambda"},{"location":"quickstart/intro/#deploying-the-project","text":"We'll now actually deploy our project to production. Unlike with many conventional build systems, building and deploying to production doesn't take very long in Refinery. Note Refinery makes heavy use of dependency caching. The first time you add or change the Import(s) that you Lambda uses you will notice that deploying and running your Lambdas will take slightly longer. This is because Refinery builds Lambda \"templates\" out of these libraries and then injects your code into the cached templates during deployment/running. Once the cache has been built all future runs and deploys will take seconds. So entire deploys may take a minute or two the first time, but the second time they will take only a few seconds. Click the Deploy to Production button in the top right corner of the page to deploy your project. This should only take a few seconds, afterwards you'll be presented with the following: Click on the View Deployment Information button, this will take you to the Deployment Viewer. The deployment viewer allows you to see your deployed project's state and allows you to do things like view logs, debug pipelines, and see your deployment in the AWS console. Click on the Lambda you just created to see more information about it, as well as the options you can do with it. For now we're just going to run this Lambda while it's deployed. Click on the Run Lambda button to get a screen similar to the previous code editor. This is mostly the same, but you're not able to edit the code since it's currently deployed. Click Run Lambda to again run our created Lambda, you'll see the very familiar screen showing the output and the return value of the Lambda. Of course, just creating single Lambdas without any chaining doesn't show off Refinery's real power which lies in it's transitions and triggers. To show some of this off we're going to make this an API endpoint without writing any additional code!","title":"Deploying the project"},{"location":"quickstart/intro/#painless-api-endpoints","text":"Click the Tear Down Deployment button to tear down our deployed project and bring us back to the regular editor. Note Tearing down deployments in Refinery is always a very fast operation. Unlike conventional build systems and production infrastructure, there's no servers to turn down or terminate so deleting things is easy. Under the Add New Resources dropdown menu, select API Endpoint . Do so again, this time selecting API Response to add an API Response node to the canvas. We now have a bunch of nodes which we can link together. Click on the API Endpoint node and click the Add Transition button. You will now see a menu similar to the following: These default options are fine, we're adding a transition from the API Endpoint to the Lambda we created. Click the Add State Transition button to add this transition. Now, click on the Get Page Lambda node, and select Add Transition once again and under Next State , select API Response and click Add State Transition . You should now have a pipeline similar to the following: We've now done all we need to do to make this Lambda an API endpoint! Click the Deploy to Production button to deploy it. Now click View Deployment Information to see the newly-deployed project. To view your newly created API Endpoint , click the API Endpoint node and select the Deployed Endpoint URL . This link will open a new tab with your API Endpoint URL and the returned JSON response is simply what was returned from the Lambda: { status : 200 , html : User-agent : *\\nDisallow : /search\\nAllow : ...trimmed for brevity...","title":"Painless API Endpoints"},{"location":"quickstart/intro/#going-forward","text":"You've now made a simple project in Refinery, check out other parts of this documentation to create more complex pipelines!","title":"Going Forward"},{"location":"transitions/","text":"Transitions Transitions are a large part of what makes Refinery an extremely powerful platform. Transitions allow different node types to be connected in a variety of ways in order to build large pipelines. Transition Types then if else exception fan-out fan-in then then is the most simple of the transitions. It will simply always pass the return data from a node to another node as input. The only time that then will not occur is when an exception occurs (which can be caught using the exception transition). if if is useful for situations where you only want to perform a transition if the return data from a node matches a certain condition. For example, only transition if the returned array or list has a length greater than zero. This transition carries the unique property of having a Conditional Expression . These are expressions which are written in Python that will cause the transition to occur if the expression returns a value of True . else else is a conditional transition which will execute if no other conditions are valid. For example, if a node has a then transition along with an if transition with a condition expression of len(return_data) 0 and the return_data has a length of 0 , the then transition would be followed. exception The exception transition is followed when the base Lambda raises an exception which is uncaught. This can be useful for situations where alerting on exceptions is necessary, or situations where recovering from a specific exception is necessary. The following is an example of the data which is returned in an exception transition: { input_data : 0 , version : 1.0.0 , exception_text : Traceback (most recent call last):\\n File \\ /var/task/lambda.py\\ , line 1089, in _init\\n return_data = main( lambda_input, context )\\n File \\ /var/task/lambda.py\\ , line 4, in main\\n return ( 100 / lambda_input )\\nZeroDivisionError: integer division or modulo by zero\\n } You can use this information to react differently depending on the specific details of the exception. Note An exception transition will result in the Lambda execution not being marked as failed in the debugging view. fan-out The fan-out transition takes a return value of a list of items and invokes the connected block with each item in the list as a single input. For example, if a Lambda block returned an array of [1, 2, 3, 4, 5] the next Lambda linked with the fan-out transition would be called five times with the inputs of 1 , 2 , 3 , 4 , and 5 respectively. This allows for performing quick and simple concurrent processing of data. Warning The fan-out transition is a powerful construct. It allows for developers to easily perform a large number of Lambda invocations without much effort. If not used carefully it can result in an excessive usage of Lambdas, causing a higher-than-expected AWS bill. It should also be noted that you still have to obey your AWS account's Lambda concurrency limits , which may result in throttling . fan-in The fan-in transition is the sister-transition of the fan-out transition. Using fan-in you can take the output of all of the concurrently invoked Lambdas and pass the return values as an array to a single Lambda. For example, if a fan-out transition executes three Lambdas concurrently and they return 1 , 2 , and 3 the input to the node connected by the fan-in transition will be [2, 1, 3] . Warning If any uncaught exceptions occur during a fan-out chain the fan-in transition will fail to execute. It's important to ensure that all exceptions are caught to prevent breaking a down-stream fan-in transition. Note The order of the returned values in the array is non-deterministic. Note You can also have multiple nodes chained together before ending in a fan-in transition (instead of just a fan-out to a Lambda node to a fan-in ). However, if all of the Lambdas do not fan-in properly the transition will fail. This can occur in situations such as a Lambda reaching its max execution time, for example.","title":"Transitions"},{"location":"transitions/#transitions","text":"Transitions are a large part of what makes Refinery an extremely powerful platform. Transitions allow different node types to be connected in a variety of ways in order to build large pipelines.","title":"Transitions"},{"location":"transitions/#transition-types","text":"then if else exception fan-out fan-in","title":"Transition Types"},{"location":"transitions/#then","text":"then is the most simple of the transitions. It will simply always pass the return data from a node to another node as input. The only time that then will not occur is when an exception occurs (which can be caught using the exception transition).","title":"then"},{"location":"transitions/#if","text":"if is useful for situations where you only want to perform a transition if the return data from a node matches a certain condition. For example, only transition if the returned array or list has a length greater than zero. This transition carries the unique property of having a Conditional Expression . These are expressions which are written in Python that will cause the transition to occur if the expression returns a value of True .","title":"if"},{"location":"transitions/#else","text":"else is a conditional transition which will execute if no other conditions are valid. For example, if a node has a then transition along with an if transition with a condition expression of len(return_data) 0 and the return_data has a length of 0 , the then transition would be followed.","title":"else"},{"location":"transitions/#exception","text":"The exception transition is followed when the base Lambda raises an exception which is uncaught. This can be useful for situations where alerting on exceptions is necessary, or situations where recovering from a specific exception is necessary. The following is an example of the data which is returned in an exception transition: { input_data : 0 , version : 1.0.0 , exception_text : Traceback (most recent call last):\\n File \\ /var/task/lambda.py\\ , line 1089, in _init\\n return_data = main( lambda_input, context )\\n File \\ /var/task/lambda.py\\ , line 4, in main\\n return ( 100 / lambda_input )\\nZeroDivisionError: integer division or modulo by zero\\n } You can use this information to react differently depending on the specific details of the exception. Note An exception transition will result in the Lambda execution not being marked as failed in the debugging view.","title":"exception"},{"location":"transitions/#fan-out","text":"The fan-out transition takes a return value of a list of items and invokes the connected block with each item in the list as a single input. For example, if a Lambda block returned an array of [1, 2, 3, 4, 5] the next Lambda linked with the fan-out transition would be called five times with the inputs of 1 , 2 , 3 , 4 , and 5 respectively. This allows for performing quick and simple concurrent processing of data. Warning The fan-out transition is a powerful construct. It allows for developers to easily perform a large number of Lambda invocations without much effort. If not used carefully it can result in an excessive usage of Lambdas, causing a higher-than-expected AWS bill. It should also be noted that you still have to obey your AWS account's Lambda concurrency limits , which may result in throttling .","title":"fan-out"},{"location":"transitions/#fan-in","text":"The fan-in transition is the sister-transition of the fan-out transition. Using fan-in you can take the output of all of the concurrently invoked Lambdas and pass the return values as an array to a single Lambda. For example, if a fan-out transition executes three Lambdas concurrently and they return 1 , 2 , and 3 the input to the node connected by the fan-in transition will be [2, 1, 3] . Warning If any uncaught exceptions occur during a fan-out chain the fan-in transition will fail to execute. It's important to ensure that all exceptions are caught to prevent breaking a down-stream fan-in transition. Note The order of the returned values in the array is non-deterministic. Note You can also have multiple nodes chained together before ending in a fan-in transition (instead of just a fan-out to a Lambda node to a fan-in ). However, if all of the Lambdas do not fan-in properly the transition will fail. This can occur in situations such as a Lambda reaching its max execution time, for example.","title":"fan-in"},{"location":"what-is-refinery/","text":"What is Refinery? Refinery is a visual IDE which allows you to build serverless services without having to deal with all of the normal complexity involved. Refinery instead opts for a more intuitive approach to development by allowing developers to build small functional programs which are linked together in a visual designer to build infinitely scalable applications. By using this approach, developers can immediately start building scalable services without any of the conventional system operations, distributed computing, and computer science knowledge. Example screenshot of a Refinery project","title":"What is Refinery?"},{"location":"what-is-refinery/#what-is-refinery","text":"Refinery is a visual IDE which allows you to build serverless services without having to deal with all of the normal complexity involved. Refinery instead opts for a more intuitive approach to development by allowing developers to build small functional programs which are linked together in a visual designer to build infinitely scalable applications. By using this approach, developers can immediately start building scalable services without any of the conventional system operations, distributed computing, and computer science knowledge. Example screenshot of a Refinery project","title":"What is Refinery?"}]}