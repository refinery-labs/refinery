#!/usr/bin/env python

"""
Copyright (C) Refinery Labs Inc. - All Rights Reserved

NOTICE: All information contained herein is, and remains
the property of Refinery Labs Inc. and its suppliers,
if any. The intellectual and technical concepts contained
herein are proprietary to Refinery Labs Inc.
and its suppliers and may be covered by U.S. and Foreign Patents,
patents in process, and are protected by trade secret or copyright law.
Dissemination of this information or reproduction of this material
is strictly forbidden unless prior written permission is obtained
from Refinery Labs Inc.
"""

_VERSION = "1.0.0"

# This is overridden in the free-tier usage case.
# In that case there is a server-side limit which
# will be obeyed.
_QUEUE_METADATA_EXPIRATION = ( 60 * 60 * 24 )

# The max size to store in redis for free-tier users
# before switching to S3 as the backing-store for 
# passing data between Lambdas. The trade off for
# using S3 is cost-per-write (PUT) and speed (since we
# have to use redis and S3). We can't use S3 alone
# because it won't grant us idempotency since it is
# eventually consistent.

# We subtract 100 bytes here to account for the redis
# key in the total size (this is counted as alloc in
# the server-side sandboxing).
_FREE_TIER_MAX_REDIS_SIZE = ( 1024 * 100 ) - 100

"""
Base redis connection for pulling configs/etc.

Allows for single-millisecond retrieval and setting
of data in the main redis memory cache.
"""
import os
import sys
import math
import uuid
import json
import time
import redis
import boto3
import base64
import urllib3
import datetime
import StringIO
import traceback
import threading
import functools
import subprocess

from urlparse import parse_qs
from websocket import create_connection

# For requests to AWS custom runtime API
http = urllib3.PoolManager()

# For the locked-down redis free tier server
# this maps redis commands to their equivalent
# EVALSHA commands.
COMMAND_MAP = {}

def hook_function_pre(function, prefunction):
	"""
	Hook a function and call a newly-added
	function before executing the hooked
	function. The hooked function is passed
	the return data of the newly-added function
	as its input.
	"""
	@functools.wraps(function)
	def run(*args, **kwargs):
		mutated_args_dict = prefunction(
			args,
			kwargs
		)

		return function(
			*mutated_args_dict["args"],
			**mutated_args_dict["kwargs"]
		)

	return run

def hook_function_post(function, postfunction):
	"""
	Hook a function and call another function with
	the output of the hooked function as input to
	the newly-added function. The return data for 
	the hook function will be the value returned
	from the newly-added function.
	"""
	@functools.wraps(function)
	def run(*args, **kwargs):
		return_value = function(
			args,
			kwargs
		)

		return postfunction(
			return_value
		)

	return run

def convert_command_to_evalsha(args, kwargs):
	"""
	This automatically converts redis commands to
	the appropriate EVALSHA calls (which allows us
	to sandbox free-tier usage of redis).
	"""

	# Convert command to EVALSHA equivalent
	command = args[0].upper()

	if not command in COMMAND_MAP:
		return {
			"args": args,
			"kwargs": kwargs
		}

	# Pull out the command args
	remaining_args = args[1:]

	new_args = (
		"EVALSHA",
		COMMAND_MAP[command],
		len( remaining_args )
	) + remaining_args

	return {
		"args": new_args,
		"kwargs": kwargs
	}

def convert_pipeline_to_evalsha(args, kwargs):
	# Pull out commands
	command_sets = args[1:][0]

	new_command_sets = []

	for command_set in command_sets:
		command = command_set[0][0]

		# If it's not in our command map we
		# just use it without mapping it
		if not command in COMMAND_MAP:
			new_command_sets.append(
				command_set
			)
			continue

		remaining_args = command_set[0][1:]
		new_inner_command_set = (
			"EVALSHA",
			COMMAND_MAP[command],
			len( remaining_args )
		) + remaining_args

		new_command_set = (
			new_inner_command_set,
			command_set[-1]
		)

		new_command_sets.append(
			new_command_set
		)

	new_args = (
		args[0],
		new_command_sets,
		args[-1]
	)

	return {
		"args": new_args,
		"kwargs": kwargs
	}

def convert_pipeline(input_pipeline):
	input_pipeline._execute_pipeline = hook_function_pre(
		input_pipeline._execute_pipeline,
		convert_pipeline_to_evalsha
	)

	return input_pipeline

def patch_redis_client( input_redis_client ):
	input_redis_client.execute_command = hook_function_pre(
		input_redis_client.execute_command,
		convert_command_to_evalsha
	)

	input_redis_client.pipeline = hook_function_post(
		input_redis_client.pipeline,
		convert_pipeline
	)

	return input_redis_client

def is_free_tier_deployment():
	"""
	Return True if this Lambda was deployed with env
	vars setting it as running in the free-tier. This
	changes how a variety of pieces work for the
	custom runtime. Mainly how redis is used and how
	data is passed between different Lambdas.
	"""
	return (
		"ACCOUNT_TYPE" in os.environ
		and os.environ[ "ACCOUNT_TYPE" ] == "FREE"
	)

class AlreadyInvokedException( Exception ):
	def __init__( self, message="This function has already been invoked!" ):
		# Call the base class constructor with the parameters it needs
		super( AlreadyInvokedException, self ).__init__( message )

class InvokeQueueEmptyException( Exception ):
	def __init__( self, message="We've exhausted all of the invocations we have to do!" ):
		# Call the base class constructor with the parameters it needs
		super( InvokeQueueEmptyException, self ).__init__( message )

class Refinery_Memory:
	# 6 hours is the timeout here because it's the max retry
	# time for async Lambda invokes:
	# "If your Lambda function is invoked asynchronously and is throttled,
	# AWS Lambda automatically retries the throttled event for up to six
	# hours, with delays between retries."
	# https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html#throttling-behavior
	return_data_timeout = ( 60 * 60 * 6 )

	json_types = [
		list,
		dict
	]

	regular_types = [
		str,
		int,
		float,
		complex,
		bool,
	]

	def __init__( self ):
		self.redis_client = False

		# New redis ACL has usernames, set it if the Lambda
		# is configured to user it.
		if "REDIS_USERNAME" in os.environ:
			self.username = os.environ[ "REDIS_USERNAME" ]

		self.hostname = os.environ[ "REDIS_HOSTNAME" ]
		self.password = os.environ[ "REDIS_PASSWORD" ]
		self.port = int( os.environ[ "REDIS_PORT" ] )

	def connect( self ):
		"""
		For free-tier users the shared locked-down redis server.

		For all other customers route them to their dedicated
		redis instance(s). 
		"""
		if is_free_tier_deployment():
			global COMMAND_MAP
			self.redis_client = redis.StrictRedis(
				username=self.username,
				host=self.hostname,
				port=self.port,
				db=0,
				socket_timeout=10,
				password=self.password,
			)

			COMMAND_MAP = self.redis_client.hgetall(
				"REFINERY_INTERNAL:COMMAND_MAP"
			)

			# Hook all commands to use the command map
			self.redis_client = patch_redis_client(
				self.redis_client
			)
			return

		self.redis_client = redis.StrictRedis(
			host=self.hostname,
			port=self.port,
			db=0,
			socket_timeout=10,
			password=self.password,
		)

	def _get_input_data_from_redis( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()

		pipeline = self.redis_client.pipeline()
		pipeline.get( key )
		pipeline.delete( key )
		returned_data = pipeline.execute()

		# Raise an exception if the GET fails
		# This is how we achieve idempotency
		if returned_data[ 0 ] == None:
			raise AlreadyInvokedException()

		try:
			return json.loads( returned_data[ 0 ] )
		except:
			pass

		return returned_data[ 0 ]

	def _store_return_data_to_redis( self, return_data, **kwargs ):
		if not self.redis_client:
			self.connect()

		new_key = str( uuid.uuid4() )

		self.redis_client.setex(
			new_key,
			self.return_data_timeout,
			json.dumps(
				return_data
			)
		)

		return new_key
		
	def _get_queue_input_from_redis( self, queue_insert_id, pop_number ):
		"""
		Pull pop_number of items off of the redis list to be thrown onto SQS.
		"""
		if not self.redis_client:
			self.connect()
			
		# List of items pulled from redis
		return_items = []

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		for i in range( 0, pop_number ):
			pipeline.lpop( "SQS_QUEUE_DATA_" + queue_insert_id )

		returned_data_list = pipeline.execute()
		
		for return_data in returned_data_list:
			if return_data != None:
				try:
					return_data = json.loads(
						return_data
					)
				except:
					pass
				return_items.append(
					return_data
				)

		return return_items
		
	def _get_queue_url( self, queue_insert_id ):
		if not self.redis_client:
			self.connect()
			
		return self.redis_client.get(
			"SQS_TARGET_QUEUE_URL_" + queue_insert_id
		)
		
	def _get_queue_backpack( self, queue_insert_id ):
		if not self.redis_client:
			self.connect()
			
		return json.loads(
			self.redis_client.get(
				"SQS_BACKPACK_DATA_" + queue_insert_id
			)
		)
		
	def _set_queue_data( self, queue_url, queue_items, backpack ):
		if not self.redis_client:
			self.connect()
			
		# Generate a unique ID
		queue_insert_id = str( uuid.uuid4() )
			
		# Generate a queue storage ID
		queue_storage_id = "SQS_QUEUE_DATA_" + queue_insert_id
		
		# Generate a redis key for storing the queue URL
		sqs_target_queue_url = "SQS_TARGET_QUEUE_URL_" + queue_insert_id
		
		# Generate a redis key for the backpack
		sqs_backpack_key = "SQS_BACKPACK_DATA_" + queue_insert_id
		
		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()
		
		# JSON-encode queue items
		for i in range( 0, len( queue_items ) ):
			queue_items[i] = json.dumps(
				queue_items[i]
			)

		# Set the queue URL
		pipeline.setex(
			sqs_target_queue_url,
			_QUEUE_METADATA_EXPIRATION, # 1-Day expiration
			queue_url
		)
		
		# Set the backpack
		pipeline.setex(
			sqs_backpack_key,
			_QUEUE_METADATA_EXPIRATION, # 1-Day expiration
			json.dumps(
				backpack
			)
		)
		
		# Set expiration of items
		pipeline.expire(
			queue_storage_id,
			_QUEUE_METADATA_EXPIRATION, # 1-Day expiration
		)
		
		pipeline.execute()
		
		# This is the batch size (the amount of queue items
		# that we put into Redis in a single transaction)
		SQS_BATCH_SIZE = ( 1000 * 10 )

		# We have to batch these up so that we can process
		# extremely large return data.
		while len( queue_items ) > 0:
			# Current batch of items to be put in redis
			queue_current_item_batch = queue_items[:SQS_BATCH_SIZE]
			
			# Remove the items from the queue items
			queue_items = queue_items[SQS_BATCH_SIZE:]
			
			# Do a redis pipeline transaction
			pipeline = self.redis_client.pipeline()

			for queue_current_item_batch_value in queue_current_item_batch:
				# Store all the queue data in redis in a list
				pipeline.rpush(
					queue_storage_id,
					queue_current_item_batch_value
				)
			
			pipeline.execute()
			
		return queue_insert_id

	def _set_fan_in_data( self, fan_out_id, invocation_list, **kwargs ):
		if not self.redis_client:
			self.connect()

		# Generate an invocation array ID
		invocation_array_id = "INVOCATION_QUEUE_" + str( uuid.uuid4() )

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		# Set up the counter for fan-in
		pipeline.setex(
			"FAN_IN_COUNTER_" + fan_out_id,
			self.return_data_timeout,
			len( invocation_list )
		)

		# Store all invocation data in redis in a list
		for invocation_item in invocation_list:
			pipeline.rpush(
				invocation_array_id,
				invocation_item
			)
		
		# Set expiration of items
		pipeline.expire(
			invocation_array_id,
			_QUEUE_METADATA_EXPIRATION, # 1-Day expiration
		)

		returned_data = pipeline.execute()

		return invocation_array_id

	def _get_invocation_input_from_queue( self, invocation_id ):
		"""
		Pops an invocation input off of the array.

		If the array has been exhausted then it raises InvokeQueueEmptyException
		"""
		if not self.redis_client:
			self.connect()

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		pipeline.lpop( invocation_id )

		returned_data = pipeline.execute()

		if returned_data[0] == None:
			raise InvokeQueueEmptyException()

		return json.loads( returned_data[0] )

	def _fan_in_get_results_data( self, fan_out_id, **kwargs ):
		"""
		Gets the fan-in data and deletes it immediately
		"""
		if not self.redis_client:
			self.connect()

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		# Get array of returned data from fan-in
		pipeline.lrange(
			"FAN_IN_RESULTS_" + fan_out_id,
			0,
			-1
		)

		# Delete the return data
		pipeline.delete(
			"FAN_IN_RESULTS_" + fan_out_id
		)

		returned_data = pipeline.execute()

		# Raise an exception if the GET fails
		# This is how we achieve idempotency
		if returned_data[ 0 ] == None:
			raise AlreadyInvokedException()

		# Decode returned data
		decoded_return_data = []
		
		# Combine all of the collective backpacks into one
		# final backpack. Colliding keys are overwritten.
		combined_backpack = {}

		for returned_data_segment in returned_data[0]:
			returned_data_dict = json.loads(
				returned_data_segment
			)
			
			if type( returned_data_dict ) == dict:
				for backpack_key, backpack_value in returned_data_dict[ "backpack" ].iteritems():
					combined_backpack[ backpack_key ] = backpack_value
			
			# Try to JSON-decode each segment
			decoded_return_data.append(
				returned_data_dict[ "output" ]
			)

		return {
			"input_data": decoded_return_data,
			"backpack": combined_backpack
		}

	def _fan_in_operations( self, fan_out_id, return_data, backpack, **kwargs ):
		"""
		This function either returns False if we're not the last Lambda
		in the fan-in to be invoked or it returns a key of the data to be
		passed to the next function after the fan-in as input.
		"""
		if not self.redis_client:
			self.connect()

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		# Push return data into redis
		pipeline.lpush(
			"FAN_IN_RESULTS_" + fan_out_id,
			json.dumps({
				"backpack": backpack,
				"output": return_data
			})
		)

		# Update expiration
		pipeline.expire(
			"FAN_IN_RESULTS_" + fan_out_id,
			self.return_data_timeout,
		)

		# Decrement fan-in counter
		pipeline.decr(
			"FAN_IN_COUNTER_" + fan_out_id
		)

		# Get fan-in counter latest value
		pipeline.get(
			"FAN_IN_COUNTER_" + fan_out_id
		)

		returned_data = pipeline.execute()

		# Check fan-in counter value
		# If it's zero, we must move to invoke the next function
		fan_in_counter_result = int( returned_data[3] )

		if fan_in_counter_result == 0:
			# Clean up previous data
			pipeline = self.redis_client.pipeline()

			# Delete fan-in counter
			pipeline.delete(
				"FAN_IN_COUNTER_" + fan_out_id
			)

			returned_data = pipeline.execute()

			return "FAN_IN_RESULTS_" + fan_out_id

		return False

	def _bulk_store_input_data( self, lambda_invocation_list ):
		"""
		Uses a redis pipeline to quickly store all the input data
		in the input lambda_invocation_list and replace "input_data"
		with a redis key for each.
		"""
		if not self.redis_client:
			self.connect()

		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		for i in range( 0, len( lambda_invocation_list ) ):
			new_input_data_key = str( uuid.uuid4() )

			payload_data = json.dumps({
				"input_data": lambda_invocation_list[ i ][ "input_data" ],
				"backpack": lambda_invocation_list[ i ][ "backpack" ]
			})

			# If we're free-tier and the return data is over
			# the maximum redis allocation size, then we write
			# the return payload to S3 instead and store the
			# object key in redis instead.
			if is_free_tier_deployment() and len( payload_data ) > _FREE_TIER_MAX_REDIS_SIZE:
				# TODO: If logging is enabled, use the log file
				# for message passing.
				payload_data = store_input_data_in_s3(
					payload_data
				)
				payload_data = json.dumps( payload_data )

			pipeline.setex(
				new_input_data_key,
				self.return_data_timeout,
				payload_data
			)

			del lambda_invocation_list[ i ][ "input_data" ]
			lambda_invocation_list[ i ][ "input_data_key" ] = new_input_data_key

		# Execute in one quick transaction
		returned_data = pipeline.execute()

		return lambda_invocation_list

	def _store_and_check_if_merge( self, hset_name, branch_ids, current_lambda_arn, merge_lambda_arns, backpack, return_data ):
		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()

		# Store return data in HASH
		pipeline.hset(
			hset_name,
			current_lambda_arn,
			json.dumps({
				"backpack": backpack,
				"return_data": return_data,
				"branch_ids": branch_ids,
			})
		)

		# Set expiration for the HASH
		pipeline.expire(
			hset_name,
			_QUEUE_METADATA_EXPIRATION
		)

		# Get the keys of the HASH after our HSET to see if we're the final
		# Lambda in the merge set.
		pipeline.hkeys(
			hset_name
		)

		# Execute in one quick transaction
		hset_returned_data = pipeline.execute()

		# Determine if we've the last Lambda in the merge
		# by the number of merge Lambdas and comparing it to the number of
		# HASH keys returned from HKEYS
		lambda_return_values_list = hset_returned_data[2]

		return len( lambda_return_values_list ) == len( merge_lambda_arns )

	def _get_merge_data( self, hset_name, branch_ids ):
		# Since we're the last Lambda in the merge we'll pull it all out.
		pipeline = self.redis_client.pipeline()
		pipeline.hvals(
			hset_name
		)
		pipeline.delete(
			hset_name
		)
		returned_data = pipeline.execute()
		lambda_return_metadata_list = returned_data[0]

		# Combine the backpacks and generate a list of return values
		return_values = []
		combined_backpack = {}

		for lambda_return_metadata in lambda_return_metadata_list:
			returned_data_dict = json.loads(
				lambda_return_metadata
			)

			for branch_id in returned_data_dict[ "branch_ids" ]:
				if not ( branch_id in branch_ids ):
					branch_ids.insert(0, branch_id)

			for backpack_key, backpack_value in returned_data_dict[ "backpack" ].iteritems():
				combined_backpack[ backpack_key ] = backpack_value

			return_values.append(
				returned_data_dict[ "return_data" ]
			)

		return {
			"backpack": combined_backpack,
			"return_data": return_values,
			"branch_ids": branch_ids,
		}

	def _merge_store_result( self, execution_id, branch_ids, target_lambda_arn, current_lambda_arn, merge_lambda_arns, backpack, return_data ):
		"""
		Uses a redis pipeline to store the results at an HASH of the following name:
		{{EXECUTION_ID}}{{TARGET_LAMBDA_ARN}} = {
			"{{CURRENT_RUNNING_LAMBDA_ARN}}": "{{RETURN_DATA}}"
		}

		If we're the last Lambda in the merge then we return the return_data to be
		passed as input to the target Lambda for the merge. Otherwise we return None.
		"""
		if not self.redis_client:
			self.connect()

		# Create HSET name
		hset_name = execution_id + "_" + target_lambda_arn + "_" + branch_ids[-1]

		# Store the merge data and check if we should merge
		should_merge = self._store_and_check_if_merge(
			hset_name,
			branch_ids,
			current_lambda_arn,
			merge_lambda_arns,
			backpack,
			return_data
		)

		# If we don't need to merge, we can stop here.
		if not should_merge:
			return None

		# Pull the merge data from redis and return it
		return self._get_merge_data(
			hset_name,
			branch_ids
		)

def get_json_from_s3( s3_bucket, s3_path ):
	s3_client = boto3.client( "s3" )

	# Pull object data from S3
	object_data = s3_client.get_object(
		Bucket=s3_bucket,
		Key=s3_path
	)

	# Return the decoded object JSON body data
	return json.loads(
		object_data[ "Body" ].read()
	)

def store_input_data_in_s3( input_payload_string ):
	"""
	Write the input data to S3.
	"""
	s3_client = boto3.client( "s3" )
	s3_object_key = str( uuid.uuid4() )
	s3_response = s3_client.put_object(
		Bucket=os.environ[ "RETURN_DATA_BUCKET_NAME" ],
		Key=s3_object_key,
		Body=input_payload_string
	)

	# We return the format a regular redis store action
	# would use because we do a redis retrieval before
	# doing the S3 pull (for idempotency).
	return {
		"input_data": {
			"_refinery": {
				"indirect": {
					"type": "redis_s3",
					"s3_bucket": os.environ[ "RETURN_DATA_BUCKET_NAME" ],
					"s3_path": s3_object_key
				}
			}
		},
		# It's fine to set this to be empty, it will be
		# replaced by the data in S3 upon unzipping.
		"backpack": {}
	}

def thread_upload_return_data_to_s3( invocation_data, invocation_list ):
	s3_ref_metadata = store_input_data_in_s3(
		json.dumps({
			"input_data": invocation_data[ "input_data" ],
			"backpack": invocation_data[ "backpack" ],
		})
	)
	"""
	lambda_input = stored_input[ "input_data" ]
	backpack = stored_input[ "backpack" ]
	"""

	# Delete input data since it's indirect
	del invocation_data[ "input_data" ]

	"""
	# Does it have an indirect type key?
	try:
		input_data_type = input_data[ "_refinery" ][ "indirect" ][ "type" ]
	except KeyError:
		return False

	# Does that type key match the external S3 format?
	return input_data_type == "redis_s3"
	"""

	# Update invocation data
	invocation_data[ "indirect" ] = s3_ref_metadata[ "input_data" ][ "_refinery" ][ "indirect" ]
	invocation_list.append(invocation_data)

def convert_invoke_data_list_into_s3_extended( invocation_list ):
	# List with big return items moved into S3 to return
	modified_invocation_list = []

	# List of active threads
	active_threads = []

	# Maximum number of threads
	# TODO: Implement
	max_threads = 50

	# For every 
	for invocation_data in invocation_list:
		# input_data[ "_refinery" ][ "indirect" ][ "type" ] == "redis_s3"

		# If it's too large for redis we upload it to S3
		# and replace the return data with a reference to
		# the object in the bucket.
		if is_free_tier_deployment() and len( json.dumps( invocation_data[ "input_data" ] ) ) > _FREE_TIER_MAX_REDIS_SIZE:
			new_thread = threading.Thread(
				target=thread_upload_return_data_to_s3,
				args=(
					invocation_data,
					modified_invocation_list
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
			active_threads.append(
				new_thread
			)
			continue

		modified_invocation_list.append(
			invocation_data
		)

	# Wait till all threads finish
	for thread in active_threads:
		thread.join()

	print( "Final invocation list: " )
	_pprint( modified_invocation_list )

	return modified_invocation_list

def get_branch_id_from_invoke_queue( invoke_queue ):
	"""
	This takes an invoke queue and analyzes it to see
	if we need to produce a new branch_id. If we do then
	it returns a new branch_id, if not it returns False.
	"""

	# Types of transitions to be counted for merges
	countable_merge_transition_types = [
		"lambda",
		"sns_topic",
		"sqs_queue",
		"merge"
	]

	# Count of the number of spawns (that match our types)
	number_of_spawns = 0

	for invoke_queue_item in invoke_queue:
		if invoke_queue_item[ "type" ] in countable_merge_transition_types:
			number_of_spawns += 1

	# If we're not branching no new branch ID is needed and 
	# we just return a False
	if number_of_spawns <= 1:
		return False

	# If the invoke_queue has more than one item it means we are branching
	# so we'll add another branch_id to the branch_ids list.
	return str( uuid.uuid4() )

def get_merge_invoke_data( new_invoke_data ):
	merge_result = gmemory._merge_store_result(
		new_invoke_data[ "execution_id" ],
		new_invoke_data[ "branch_ids" ],
		new_invoke_data[ "arn" ],
		new_invoke_data[ "invoked_function_arn" ],
		new_invoke_data[ "merge_lambdas" ],
		new_invoke_data[ "backpack" ],
		new_invoke_data[ "input_data" ]
	)

	if merge_result == None:
		return merge_result

	# Pull the shared branch IDs from redis
	new_invoke_data[ "branch_ids" ] = merge_result[ "branch_ids" ]

	# Remove the last branch ID
	new_invoke_data[ "branch_ids" ] = new_invoke_data[ "branch_ids" ][1:]

	# If the return value was not None that means it's the final
	# Lambda in the merge and we can do a "then" transition with
	# the combined array of results as the input.
	new_invoke_data = {
		"branch_ids": new_invoke_data[ "branch_ids" ],
		"backpack": merge_result[ "backpack" ],
		"execution_id": new_invoke_data[ "execution_id" ],
		"fan_out_ids": new_invoke_data[ "fan_out_ids" ],
		"type": "merge",
		"arn": new_invoke_data[ "arn" ],
		"input_data": json.loads(
			json.dumps(
				merge_result[ "return_data" ]
			)
		)
	}

	return new_invoke_data
		
# Make global to enable caching of redis connection
gmemory = Refinery_Memory()

def _parallel_invoke( branch_ids, invoke_queue ):
	# List of active threads
	active_threads = []

	# Maximum number of threads
	max_threads = 50

	# Invokes a Lambda directly with input_data asynchronously
	def lambda_invoker_worker_direct_input( execution_id, branch_ids, fan_out_ids, arn, input_data ):
		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"input_data": input_data,
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}

		lambda_client = boto3.client(
			"lambda"
		)

		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit

	# Invokes a Lambda asynchronously
	def lambda_invoker_worker_redis( execution_id, branch_ids, fan_out_ids, arn, return_key ):
		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"indirect": {
					"type": "redis",
					"key": return_key,
				},
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}

		lambda_client = boto3.client(
			"lambda"
		)

		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit

	# Invokes a Lambda asynchronously
	def lambda_invoker_worker_redis_s3( execution_id, branch_ids, fan_out_ids, arn, s3_bucket, s3_path ):
		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"indirect": {
					"type": "redis_s3",
					"s3_bucket": s3_bucket,
					"s3_path": s3_path,
				},
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}

		lambda_client = boto3.client(
			"lambda"
		)

		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit

	# Self-invocation for a fan-out transition
	def lambda_invoker_fan_out( arn, return_key, invocation_id, branch_ids ):
		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"invoke": invocation_id,
				"indirect": {
					"type": "redis",
					"key": return_key,
				},
				"parallel": False
			}
		}

		lambda_client = boto3.client(
			"lambda"
		)

		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit

	# Invokes a Lambda asynchronously with fan-in results as input
	def lambda_invoker_worker_fan_in( execution_id, branch_ids, fan_out_ids, arn, fan_out_id ):
		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"indirect": {
					"type": "redis_fan_in",
					"key": fan_out_id,
				},
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}

		lambda_client = boto3.client(
			"lambda"
		)

		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit
		
	# Mass-inserts return data into SQS queue
	def sqs_insert_data( execution_id, branch_ids, fan_out_ids, arn, input_data, backpack, context ):
		# Generate queue URL from SQS ARN
		arn_parts = arn.split( ":" )
		queue_url = "https://sqs." + arn_parts[3] + ".amazonaws.com/" + arn_parts[4] + "/" + arn_parts[5]
		
		queue_items = input_data
		
		if type( input_data ) != list:
			queue_items = [
				input_data
			]
		
		# Insert all the data into redis as a temporary holding zone
		queue_insert_id = gmemory._set_queue_data(
			queue_url,
			queue_items,
			backpack
		)
		
		# We calculate the number of workers to spin up by
		# having a target of 1 minute of time to fill up the SQS
		# queue with the messages previously stored in redis.
		# This is to balance the additional invoke and compute
		# cost with the speed of insertion into SQS.
		# A multi-threaded SQS worker generally can insert at a
		# rate of 400 messages per second.
		
		number_of_workers = len( queue_items ) / ( 400 * 60 ) # 400/sec for 60 seconds
		number_of_workers = int( math.ceil( number_of_workers ) )
		
		# We cap the number of workers at 20 just to make the cost
		# reasonable
		if number_of_workers > 20:
			number_of_workers = 20
			
		# If the float is to small it will occassionally be brought
		# down to zero so we set a minimum of one worker
		if number_of_workers < 1:
			number_of_workers = 1
		
		lambda_client = boto3.client(
			"lambda"
		)
		
		# Invoke data for the self-invokes to spawn SQS workers
		for i in range( 0, number_of_workers ):
			lambda_invoke_data = {
				"_refinery": {
					"sqs_worker": {
						"branch_ids": branch_ids,
						"queue_insert_id": queue_insert_id,
						"execution_id": execution_id,
						"fan_out_ids": fan_out_ids,
					}
				}
			}
			
			response = lambda_client.invoke(
				FunctionName=context.invoked_function_arn,
				InvocationType="Event",
				LogType="None",
				Payload=json.dumps(
					lambda_invoke_data
				)
			)
			
			# Check how much runway we have. If it's less than five seconds
			# we'll dip out and it'll just have to be a slower drip into SQS.
			# This is after the first-invoke intentionally so that we always
			# have at least one going.
			if context.get_remaining_time_in_millis() <= ( 5 * 1000 ):
				break
	
		raise SystemExit

	# Publishes to an SNS topic
	def sns_topic_publish_worker( execution_id, branch_ids, fan_out_ids, arn, input_data, backpack ):
		# Handle large data that won't fit in the 256K SNS max size
		sns_client = boto3.client(
			"sns"
		)

		return_wrapper_data = {
			"_refinery": {
				"branch_ids": branch_ids,
				"indirect": False,
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"input_data": input_data,
				"backpack": backpack
			}
		}

		response = sns_client.publish(
			TopicArn=arn,
			Message=json.dumps(
				return_wrapper_data
			)
		)

		raise SystemExit

	# Publishes to redis to be picked up by a polling API Gateway Lambda
	def api_gateway_response( execution_id, input_data ):
		if input_data == False:
			input_data = "<FIX_REDIS_FALSE_BOOL_PASSING_ISSUE>"
		
		# Store with expiration in redis
		gmemory.redis_client.setex(
			execution_id,
			gmemory.return_data_timeout,
			json.dumps(
				input_data
			)
		)

		raise SystemExit
		
	# Spawns a new worked and adds it to active_threads
	def spawn_new_worker():
		# Types that equate to just a "lambda" type execution
		lambda_equivalent_types = [
			"lambda",
			"merge",
			"fan_out_execution"
		]

		new_invoke_data = invoke_queue.pop()

		# Check type and invoke appropriately
		if ( new_invoke_data[ "type" ] in lambda_equivalent_types ) and "input_data_key" in new_invoke_data:
			new_thread = threading.Thread(
				target=lambda_invoker_worker_redis,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data_key" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif ( new_invoke_data[ "type" ] in lambda_equivalent_types ) and "input_data" in new_invoke_data:
			new_thread = threading.Thread(
				target=lambda_invoker_worker_direct_input,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif ( new_invoke_data[ "type" ] in lambda_equivalent_types ) and "indirect" in new_invoke_data:
			# This is for nested fan-outs that are also S3 loaded
			new_thread = threading.Thread(
				target=lambda_invoker_worker_redis_s3,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "indirect" ][ "s3_bucket" ],
					new_invoke_data[ "indirect" ][ "s3_path" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "lambda_fan_out":
			new_thread = threading.Thread(
				target=lambda_invoker_fan_out,
				args=(
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data_key" ],
					new_invoke_data[ "invocation_id" ],
					new_invoke_data[ "branch_ids" ],
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "lambda_fan_in":
			new_thread = threading.Thread(
				target=lambda_invoker_worker_fan_in,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "fan_out_id" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "sns_topic":
			new_thread = threading.Thread(
				target=sns_topic_publish_worker,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ],
					new_invoke_data[ "backpack" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "api_gateway_response":
			new_thread = threading.Thread(
				target=api_gateway_response,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
		elif new_invoke_data[ "type" ] == "sqs_queue":
			new_thread = threading.Thread(
				target=sqs_insert_data,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "branch_ids" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ],
					new_invoke_data[ "backpack" ],
					new_invoke_data[ "context" ],
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug

		return new_thread

	# Bug fix
	lambda_client = boto3.client(
		"lambda"
	)

	# First iterate over all the invocations and load the input data
	# into redis in a single transaction/pipeline to speed it up.
	new_invoke_queue = []

	# Create a special list for Lambdas to load the input data at once
	lambda_invoke_list = []

	# If it's necessary, get the new branch ID
	new_branch_id = get_branch_id_from_invoke_queue(
		invoke_queue
	)

	while len( invoke_queue ) > 0:
		new_invoke_data = invoke_queue.pop()

		# Copy in the branch IDs
		new_invoke_data[ "branch_ids" ] = json.loads(
			json.dumps(
				branch_ids
			)
		)

		# If we have a new branch ID, add it to the list of branch IDs
		# This only occurs when we have a new branch of execution (e.g.
		# we have two Lambdas spawned from one Lambda).
		if new_branch_id:
			new_invoke_data[ "branch_ids" ].insert( 0, new_branch_id )

		# If it's a merge transition, then pop off the last branch ID
		if new_invoke_data[ "type" ] == "merge":
			new_invoke_data = get_merge_invoke_data(
				new_invoke_data
			)

			# If the merge result was None there's no additional invocation
			# to be made here so we can skip this loop.
			if new_invoke_data == None:
				continue

		if new_invoke_data[ "type" ] == "lambda" or new_invoke_data[ "type" ] == "lambda_fan_out":
			lambda_invoke_list.append(
				new_invoke_data
			)
		else:
			new_invoke_queue.append(
				new_invoke_data
			)

	# Process the Lambda's input_data
	# input_data is replaced with return_key
	lambda_invoke_list = gmemory._bulk_store_input_data(
		lambda_invoke_list
	)

	# Combine resulting lists
	invoke_queue = new_invoke_queue + lambda_invoke_list

	# Keep looping while there's still Lambdas to invoke
	while len( invoke_queue ) > 0:
		# If we've not maxing out acive threads then spawn new ones
		if len( active_threads ) < max_threads:
			active_threads.append(
				spawn_new_worker()
			)
		else:
			# Iterate over our active threads and remove finished
			new_active_thread_list = []
			already_spawned = False

			# Check if any threads are finished
			for active_thread in active_threads:
				if active_thread.is_alive():
					new_active_thread_list.append( active_thread )
				elif already_spawned == False:
					new_active_thread_list.append(
						spawn_new_worker()
					)
					already_spawned = True

			active_threads = new_active_thread_list

	# Wait until all threads finish
	for thread in active_threads:
		thread.join()

	# We're all done
	return

def _api_endpoint( lambda_input, execution_id, context ):
	# This will spin and continually query redis until either
	# we time out without getting our HTTP response OR we return
	# our response to the client.

	# Continually loop until we have only two seconds left
	# Max execution time is 30 seconds, so that's 28 seconds
	timed_out = True

	# As long as we have ~2 seconds of runway left continue
	# to query redis for our HTTP response data.
	while context.get_remaining_time_in_millis() > ( 2 * 1000 ):
		try:
			http_response = gmemory._get_input_data_from_redis(
				execution_id
			)
		except AlreadyInvokedException as e:
			http_response = False

		# When we have a non-False http_response we can
		# break out of the loop and declare we've not timed out.
		if http_response != False:
			timed_out = False
			break

		# Wait a moment before checking again
		time.sleep( 0.01 )
		
	# If it matches our magic value, replace the response
	# with "False" instead. This is to get around an issue with
	# the Python redis library using "False" as a "not found" default
	# value. So this is how we distinguish the difference.
	if http_response == "<FIX_REDIS_FALSE_BOOL_PASSING_ISSUE>":
		http_response = False

	# We've timed out, return an error
	if timed_out:
		return {
			"statusCode": 504,
			"headers": {},
			"body": json.dumps({
				"msg": "The request to the backend has timed out.",
				"success": False
			}),
			"isBase64Encoded": False
		}

	# Check if the response is actually in an API Gateway already
	# If not return as just regular JSON, if it is then return raw
	if type( http_response ) == dict and "body" in http_response:
		return http_response

	# Return JSON response with the data
	return {
		"statusCode": 200,
		"headers": {
			"Content-Type": "application/json",
			"X-Frame-Options": "deny",
			"X-Content-Type-Options": "nosniff",
			"X-XSS-Protection": "1; mode=block",
			"Cache-Control": "no-cache, no-store, must-revalidate",
			"Pragma": "no-cache",
			"Expires": "0",
			"Server": "refinery"
		},
		"body": json.dumps( http_response ),
		"isBase64Encoded": False
	}
	
S3_CLIENT = boto3.client(
	"s3"
)
	
def _write_temporary_execution_results( s3_log_bucket, program_output, return_data ):
	"""
	This is for getting around all of Lambda's extremely painful limitations
	when you run something manually, mainly:
	* Truncation of output data
	* Truncation of return data
	* Slow writing/flushing to Cloudwatch to retrieve output data
	
	This function is called when you pass special input to the Lambda
	(Code Block) when running it. It will cause the return data to
	indicate that the results are stored in S3.
	"""
	
	unique_id = str( uuid.uuid4() )
	
	# Full details of the execution
	temporary_run_data = {
		"program_output": program_output,
		"return_data": return_data,
	}
	
	# The S3 path we're storing the data at
	s3_object_path = "temporary_executions/" + unique_id + ".json"
	
	# Write data to S3
	response = S3_CLIENT.put_object(
		Bucket=s3_log_bucket,
		Key=s3_object_path,
		Body=json.dumps(
			temporary_run_data,
			sort_keys=True
		)
	)
	
	return {
		"s3_bucket": s3_log_bucket,
		"s3_path": s3_object_path,
	}

def _write_pipeline_logs( s3_log_bucket, project_id, lambda_arn, lambda_name, execution_pipeline_id, log_type, execution_details, program_output, backpack, input_data, return_data ):
	def get_nearest_five_minutes():
		round_to = ( 60 * 5 )
		dt = datetime.datetime.now()
		seconds = ( dt.replace( tzinfo=None ) - dt.min ).seconds
		rounding = (
						   seconds + round_to / 2
				   ) // round_to * round_to
		return dt + datetime.timedelta( 0, rounding - seconds, - dt.microsecond )

	s3_client = boto3.client(
		"s3"
	)

	log_id = str( uuid.uuid4() )

	nearest_minute = get_nearest_five_minutes()
	date_shard_string = "dt=" + nearest_minute.strftime( "%Y-%m-%d-%H-%M" )

	s3_path = project_id + "/" + date_shard_string + "/" + execution_pipeline_id + "/" + log_type + "~" + lambda_name + "~" + log_id + "~" + str( int( time.time() ) )

	s3_data = {
		"id": log_id,
		"execution_pipeline_id": execution_pipeline_id,
		"project_id": project_id,
		"arn": lambda_arn,
		"name": lambda_name,
		"type": log_type, # INPUT, EXCEPTION, COMPLETE
		"timestamp": int( time.time() ),
		"program_output": program_output,
		"backpack": backpack,
		"input_data": input_data,
		"return_data": return_data,
	}

	# Merge in execution_details
	for key, value in execution_details.iteritems():
		s3_data[ key ] = value

	response = s3_client.put_object(
		Bucket=s3_log_bucket,
		Key=s3_path,
		Body=json.dumps(
			s3_data,
			sort_keys=True
		)
	)
	
def _send_sqs_message_in_batches( sqs_url, sqs_message_list ):
	sqs_message_list = sqs_message_list
	new_sqs_client = boto3.client( "sqs" )
	
	while True:
		# Pull last ten messages off the list
		message_batch = sqs_message_list[-10:]
		
		# Remove them from the original messages
		sqs_message_list = sqs_message_list[:-10]
		
		if len( message_batch ) == 0:
			break
		
		_send_sqs_messages(
			new_sqs_client,
			sqs_url,
			message_batch
		)
		
		if len( sqs_message_list ) == 0:
			break
	
	raise SystemExit

def _send_sqs_messages( sqs_client, sqs_url, sqs_message_list ):
	sqs_entries = []
	
	# Generate SQS entries
	for sqs_message in sqs_message_list:
		sqs_entries.append({
			"Id": str( uuid.uuid4() ),
			"MessageBody": json.dumps(
				sqs_message
			)
		})
		
	response = sqs_client.send_message_batch(
		QueueUrl=sqs_url,
		Entries=sqs_entries
	)
	
def _spawn_off_sqs_worker( queue_url, queue_insert_id, execution_id, branch_ids, fan_out_ids ):
	exhausted_queue = False
	new_thread = False
	
	queue_items = gmemory._get_queue_input_from_redis(
		queue_insert_id,
		100
	)
	
	enriched_queue_items = []
	
	# Add metadata to queue items
	for queue_item in queue_items:
		enriched_queue_items.append({
			"_refinery": {
				"queue_insert_id": queue_insert_id,
				"execution_id": execution_id,
				"branch_ids": branch_ids,
				"fan_out_ids": fan_out_ids,
				"input_data": queue_item,
			}
		})
	
	if len( enriched_queue_items ) == 0:
		exhausted_queue = True
	else:
		new_thread = threading.Thread(
			target=_send_sqs_message_in_batches,
			args=(
				queue_url,
				enriched_queue_items
			)
		)
		
		new_thread.start()
		
		# Wedge for Boto3 bug: https://github.com/boto/botocore/issues/1246
		time.sleep(0.2)
	
	return {
		"thread": new_thread,
		"exhausted_queue": exhausted_queue
	}
	
def _sqs_worker( queue_insert_id, execution_id, branch_ids, fan_out_ids, context ):
	"""
	An SQS worker takes the messages stored in Redis and loads them into
	an SQS queue. The Refinery "magic" which takes place is the ability to
	return a large number of items (e.g. 100K, 1M) from a code block and have
	them all be loaded into SQS without any additional code. Of course, this is
	not quite so simple to do in implementation. SQS allows a maximum of 10 messages
	to be stored in the queue per API request. While the throughput is unlimited, it
	would almost certainly take longer than the max-timeout of a given Lambda to store
	all of this data in SQS. So instead we opt for loading all of the data into redis
	first (very fast) and then self-invoke the Lambda in the SQS Worker mode. The SQS
	Worker mode just pulls things off of the redis queue and loads them into SQS 10
	messages at a time. If the timeout for the function is close at hand the SQS worker
	will automatically finish it's current load and then invoke itself again to extend
	the work infinitely until it is done.
	
	Basically the steps are the following:
	* A Code Block returns a large array of values to be stored in SQS.
	* The custom runtime receives this array and stores it in redis as a list (fast).
	* The custom runtime self-invokes as many SQS workers as calculated to be necessary.
	* The SQS workers pull 10 messages a time out (multi-threaded) of redis and insert them into SQS
	* Upon getting close to their timeout the SQS workers finish up and then invoke themselves
	* This continues until the redis list has been exhaused.
	
	Each SQS worker can do 10 messages at a time, and this is multi-threaded.
	Performance measurements show the ideal thread number to be 15. Using this
	we can achieve about 1K SQS inserts per 2.5 seconds (for every worker instance).
	
	Assuming ~400 inserts a second, that's 24K a minute (and 360K in 15 minutes).
	5 SQS workers is 120K a minute
	10 SQS works is 240K a minute
	"""
	
	print( "I'm an SQS worker spawned with queue_insert_id of: " + queue_insert_id )
	sys.stdout.flush()
	
	# If we have less than or equal to ten seconds of
	# remaining runtime we should quit out.
	remaining_time_limit = ( 1000 * 10 )
	
	# Get target queue URL
	queue_url = gmemory._get_queue_url(
		queue_insert_id
	)
	
	# 15 is about the golden number of effective threads
	max_threads = 15
	
	while True:
		active_threads = []
		exhausted_queue = False
		
		# Spawn up threads to put things on SQS
		for i in range( 0, max_threads ):
			if exhausted_queue == False:
				worker_spawn_data = _spawn_off_sqs_worker(
					queue_url,
					queue_insert_id,
					execution_id,
					branch_ids,
					fan_out_ids
				)
				
				if worker_spawn_data[ "thread" ]:
					active_threads.append(
						worker_spawn_data[ "thread" ]
					)
					
				if worker_spawn_data[ "exhausted_queue" ]:
					exhausted_queue = True
		
		# Wait until all threads finish
		for thread in active_threads:
			thread.join()
			
		# If we've finished up our queue we should quit out
		if exhausted_queue:
			break
			
		# Now check how much time we have left before timeout
		remaining_milliseconds = context.get_remaining_time_in_millis()
		if remaining_milliseconds <= remaining_time_limit:
			# Self invoke and quit
			lambda_client = boto3.client(
				"lambda"
			)
			
			lambda_invoke_data = {
				"_refinery": {
					"sqs_worker": {
						"branch_ids": branch_ids,
						"queue_insert_id": queue_insert_id,
						"execution_id": execution_id,
						"fan_out_ids": fan_out_ids,
					}
				}
			}
			
			response = lambda_client.invoke(
				FunctionName=context.invoked_function_arn,
				InvocationType="Event",
				LogType="None",
				Payload=json.dumps(
					lambda_invoke_data
				)
			)
			break
	return

def _spawner( branch_ids, invocation_id, context ):
	"""
	Fan-out invocations work in essentially the following stages:
	* A Lambda is invoked which has a fan-out transition
	* This Lambda sets up the counter in redis to prepare for fan-in
	* The Lambda also stores all invocation data in redis as a list
	* The Lambda then invokes itself with the ["_refinery"]["invoke"] options

	The value of ["_refinery"]["invoke"] is the following:
	{
		"invoke_id": {{UUID_OF_REDIS_LIST}},
		"invoke_speed": {{INTEGER_OF_PARALELL_SPAWNERS}},
	}

	The "invoke_id" refers to the ID of the list of invoke input(s)
	stored inside of redis. The "invoke_speed" refers to how many
	"spawner" Lambdas will be invoked.

	A "spawner" Lambda is just the Lambda invoking itself into a
	specific new mode of operation. This mode of operation works
	by spawning off multiple invocation threads which are continously
	looped over and fed more invoke requests. In each loop the remaining
	execution time is also checked. If the remaining execution time is
	less than 5 seconds the Lambda will then invoke itself again once
	to continue the invocation work (and will finish out it's pending
	invocations). If the invoke queue is finished the Lambda simply
	immediately exits.

	The "invoke_speed" determines the number of "spawner" Lambdas will
	run at the same time. In any case the spawners will always invoke
	all of the requested Lambdas for the returned data. The only thing
	that varies is the speed at which this will occur. It could be at
	the speed of 10 Lambdas invoking at ~18 invokes a second, or it could
	be at 100 Lambdas at ~18 invokes a second.
	"""
	print( "I am a spawner lambda who's spawned with an invocation ID of " + invocation_id )
	sys.stdout.flush()

	remaining_time_limit = ( 1000 * 10 )
	parallel_invocation_number = 15
	queue_exhausted = False

	while queue_exhausted == False:
		# Reset list of Lambdas to invoke
		lambdas_to_invoke = []

		# Check if we're close to a timeout, if we are we should invoke ourselfs
		# and then dip out of performing more invocations.
		remaining_milliseconds = context.get_remaining_time_in_millis()
		if remaining_milliseconds <= remaining_time_limit:
			lambdas_to_invoke.append({
				"type": "lambda_fan_out",
				"arn": context.invoked_function_arn,
				"invocation_id": invocation_id,
				# Input data is purely to ensure we get idempotency
				"input_data": {
					"why": "idempotency"
				}
			})
			start_time = time.time()
			_parallel_invoke( branch_ids, lambdas_to_invoke )
			return

		# Pull at least parallel_invocation_number number of inputs of the queue
		for i in range( 0, parallel_invocation_number ):
			try:
				invocation_input = gmemory._get_invocation_input_from_queue(
					invocation_id
				)

				print( "Invocation input for spawner: " )
				print( json.dumps(invocation_input) )
				sys.stdout.flush()

				lambdas_to_invoke.append(
					invocation_input
				)
			except InvokeQueueEmptyException as e:
				print( "Queue exhausted!" )
				sys.stdout.flush()
				queue_exhausted = True
				# Break out of immediate for loop
				break

		start_time = time.time()
		_parallel_invoke( branch_ids, lambdas_to_invoke )

	return

def _pprint( input_dict ):
	try:
		print( json.dumps( input_dict, sort_keys=True, indent=4, separators=( ",", ": " ) ) )
	except:
		print( input_dict )
		
def _transition_type_in_transitions( transitions, transition_type ):
	if "then" in transitions and type( transitions[ "then" ] ) == list:
		for transition in transitions[ "then" ]:
			if "type" in transition and transition[ "type" ] == transition_type:
				return True
			
	return False
	
def _warmup_concurrency_self_invoke( arn, decrement_counter ):
	# Decrement concurrency counter
	decrement_counter = decrement_counter - 1
	
	if decrement_counter > 0:
		print( "We have a higher level of concurrency to meet (" + str( decrement_counter ) + " remaining), kicking off another layer of concurrency..." )
		sys.stdout.flush()
		
		lambda_client = boto3.client(
			"lambda"
		)
	
		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="RequestResponse",
			LogType="Tail",
			Payload=json.dumps({
				"_refinery": {
					"warmup": decrement_counter
				},
			})
		)
		return
	
	print( "We have no further concurrency to meet, this is the last one in the chain." )
	sys.stdout.flush()
	
def _improve_api_endpoint_request_data( lambda_input ):
	# Try several body decoding methods and set fields
	# with the decoded values for each.
	if "body" in lambda_input:
		# Decode base64 body
		if "isBase64Encoded" in lambda_input and lambda_input[ "isBase64Encoded" ]:
			# Base64
			try:
				lambda_input[ "raw_body" ] = base64.b64decode(
					lambda_input[ "body" ]
				)
			except Exception as e:
				pass
		else:
			lambda_input[ "raw_body" ] = lambda_input[ "body" ]
		
		# JSON
		try:
			lambda_input[ "json" ] = json.loads(
				lambda_input[ "raw_body" ]
			)
		except Exception as e:
			lambda_input[ "json" ] = None
		
		# application/x-www-form-urlencoded
		try:
			lambda_input[ "form" ] = parse_qs(
				lambda_input[ "raw_body" ]
			)
		except Exception as e:
			lambda_input[ "form" ] = None
		
	return lambda_input
	
def _write_inline_code( inline_code, path ):	
	with open( path, "w" ) as file_handler:
		file_handler.write( inline_code.encode( "utf8" ) )
	return path

def _write_s3_binary( s3_path, local_path ):
	s3_response = S3_CLIENT.get_object(
		Bucket=os.environ[ "PACKAGES_BUCKET_NAME" ],
		Key=s3_path
	)

	with open( local_path, "w" ) as file_handler:
		file_handler.write(
			s3_response[ "Body" ].read()
		)	

def _setup_inline_execution_shared_files( inline_file_list ):
	shared_files_base_folder = "/tmp/shared_files/"

	if os.path.exists( shared_files_base_folder ):
		_clear_temporary_files()

	# Make shared files directory
	os.mkdir( shared_files_base_folder )

	# Write all of the shared files
	for shared_file_metadata in inline_file_list:
		shared_file_path = shared_files_base_folder + shared_file_metadata[ "name" ]
		_write_inline_code(
			shared_file_metadata[ "body" ],
			shared_file_path,
		)

def _clear_temporary_files():
	try:
		tmp_handler = subprocess.Popen(
			[ "/bin/rm -rf /tmp/*" ],
			stdout=subprocess.PIPE,
			stderr=subprocess.PIPE,
			stdin=subprocess.PIPE,
			shell=True
		)
		tmp_handler.communicate()
	except e:
		pass
	
def _stream_execution_output_to_websocket( websocket, debug_id, output ):
	websocket.send(json.dumps({
		"version": "1.0.0",
		"debug_id": debug_id,
		"action": "OUTPUT",
		"source": "LAMBDA",
		"body": output,
		"timestamp": int( time.time() )
	}))

def is_external_s3_storage_structure( input_data ):
	# Is it a dictionary?
	if type( input_data ) != dict:
		return False

	# Does it have an indirect type key?
	try:
		input_data_type = input_data[ "_refinery" ][ "indirect" ][ "type" ]
	except KeyError:
		return False

	# Does that type key match the external S3 format?
	return input_data_type == "redis_s3"

def _init( custom_runtime, request_id, lambda_input, context, execution_details ):
	start_time = time.time()

	global gmemory
	
	# Generate the path to the execution payload
	handler_parts = os.getenv( "_HANDLER" ).split( "." )
	executable_path = os.getenv( "LAMBDA_TASK_ROOT" ) + "/" + handler_parts[0]

	execution_pipeline_id = os.environ[ "EXECUTION_PIPELINE_ID" ]
	s3_log_bucket = os.environ[ "LOG_BUCKET_NAME" ]
	execution_pipeline_logging_level = os.environ[ "PIPELINE_LOGGING_LEVEL" ]

	# If pipeline logging is enabled
	# We initialize the S3 client to prevent threading bugs
	if execution_pipeline_logging_level != "LOG_NONE":
		_start_clock( "S3 Boto Client Initialization" )
		# Bug fix test
		s3_client = boto3.client(
			"s3"
		)
		_stop_clock( "S3 Boto Client Initialization" )

	# Indicate a special execution condition
	# For example, an API Gateway Lambda
	execution_mode = os.environ[ "EXECUTION_MODE" ]

	# Branch IDs, these are used for merges and are added whenever
	# a Code Block spawns multiple child executions (through multiple
	# thens, a fan-out, etc)
	branch_ids = []

	# Execution ID, this is an ID which correlates to an execution chain
	execution_id = False

	# Bake in transition data
	transitions = json.loads( os.environ[ "TRANSITION_DATA" ] )

	# Set default fan-out IDs list to be empty
	fan_out_ids = []

	# Throw exceptions fully (for use in tmp-runs, etc)
	throw_exceptions_fully = False
	
	# If the execution is a temporary execution, for example if the
	# Lambda is being execute in the Refinery editor.
	is_temporary_execution = False
	
	# If the execution is being live streamed via WebSocket callbacks
	# then this will be the actual reference to the websocket client.
	live_debug = False
	
	# The backpack variable is a variable that is automatically passed
	# between code block executions and is an arbitrary map/dict/object
	# that you can set key/values on. This is useful for storing some data
	# to be used in a non-immediate code block in the workflow. This way you
	# don't have to always return all of your data in a weird format in order
	# to pass it along.
	backpack = {}

	_start_clock( "Loading Input Data" )
	
	# This is for the SNS topic message case, we just attempt to parse
	# the JSON at the location we expect it to be. If it fails it's just
	# because it's not a SNS message and so we can skip it.
	try:
		lambda_input = json.loads(
			lambda_input[ "Records" ][ 0 ][ "Sns" ][ "Message" ]
		)
	except:
		# If we get an exception it's just because it isn't an SNS topic message.
		# So we just continue on as usual.
		pass
	
	# This is for messages coming off an SQS queue, the regular output is confusing
	# So we format it to be regular input
	try:
		# This is just to check if it's an SQS "shaped" message
		receipt_handle = lambda_input[ "Records" ][0][ "receiptHandle" ]
		
		lambda_input_list = []
		
		# Set metadata from items in queue batch
		for batch_item in lambda_input[ "Records" ]:
			batch_input_data = json.loads(
				batch_item[ "body" ]
			)
			
			if "_refinery" in batch_input_data:
				# Set the metadata
				execution_id = batch_input_data[ "_refinery" ][ "execution_id" ]
				fan_out_ids = batch_input_data[ "_refinery" ][ "fan_out_ids" ]
				queue_insert_id = batch_input_data[ "_refinery" ][ "queue_insert_id" ]
				branch_ids = batch_input_data[ "_refinery" ][ "branch_ids" ]
				
			lambda_input_list.append(
				batch_input_data[ "_refinery" ][ "input_data" ]
			)
				
		# Let's grab the backpack
		backpack = gmemory._get_queue_backpack(
			queue_insert_id
		)
		
		# Set the Lambda input data
		lambda_input = lambda_input_list

		# If that didn't throw an exception it's certainly SQS and we need to format
		# the returned data.
	except Exception as e:
		pass
	
	# By default inline execution is disabled
	is_inline_execution = False

	print( "Raw input data to Lambda is the following: " )
	print( json.dumps(lambda_input) )
	sys.stdout.flush()

	# Detect refinery wrapper and unwrap if existant
	# Else just leave it unmodified
	if type( lambda_input ) == dict and "_refinery" in lambda_input:
		print( "Input contains _refinery, processing...")
		sys.stdout.flush()

		# If a branch ID array exists in the input, set it.
		if "branch_ids" in lambda_input[ "_refinery" ]:
			branch_ids = lambda_input[ "_refinery" ][ "branch_ids" ]

		"""
		In order to speed up inline executions (e.g. Editor executions)
		we keep the base Lambda layers/packages/configs deployed in a Lambda.
		Then we set an environment variable on the Lambda to indicate that this is
		just a Lambda to be used for inline execution. When this environment variable
		is set it allows for passing arbitrary code into the Lambda as a parameter of "inline_code":
		
		For example:
		{
			"_refinery": {
				"inline_code": "def test()..."
			}
		}
		
		This code is written to a file in /tmp/ and the "executable_path" variable
		is updated to this new path.
		
		For other Lambdas in production this environment variable is NOT set so this
		input being passed will have no effect. Obviously arbitrary code execution is
		not something we want to allow for the actually Lambdas we have in prod deploys :)
		
		This dramatically increases execution times for inline runs and allows a much better
		development experience for our users.
		"""
		is_inline_execution = (
			os.getenv( "IS_INLINE_EXECUTOR", False ) == "True" and
			"inline_code" in lambda_input[ "_refinery" ]
		)
		
		# If it's an inline execution write the temporary code file
		if is_inline_execution:
			# Backwards compatibility
			if type( lambda_input[ "_refinery" ][ "inline_code" ] ) == str:
				lambda_input[ "_refinery" ][ "inline_code" ] = {
					"shared_files": [],
					"base_code": lambda_input[ "_refinery" ][ "inline_code" ][ "base_code" ]
				}

			# Set up files for inline execution, returns executable path
			_setup_inline_execution_shared_files(
				lambda_input[ "_refinery" ][ "inline_code" ][ "shared_files" ]
			)

			# Base code path
			executable_path = "/tmp/" + str( uuid.uuid4() )
			if "base_code" in lambda_input[ "_refinery" ][ "inline_code" ]:
				_write_inline_code(
					lambda_input[ "_refinery" ][ "inline_code" ][ "base_code" ],
					executable_path
				)

			# Pull in a remote file and execute it. This is used for things like
			# inline executions using fully-compiled binaries (Go). Since passing
			# the binary in as input would exceed the max size we pass the S3 path
			# in and then pull it down at runtime.
			if "s3_path" in lambda_input[ "_refinery" ][ "inline_code" ]:
				# Write binary to disk
				_write_s3_binary(
					lambda_input[ "_refinery" ][ "inline_code" ][ "s3_path" ],
					executable_path
				)

				# Mark as executable
				os.chmod( executable_path, 0775 )
			
		"""
		For doing live debugging/streaming of the Lambda execution.
		
		Provides a debug_id for tracing and a callback URL for the websocket
		to do the actual calling-back to.
		
		"live_debug": {
			"debug_id": "{{UUID}}",
			"websocket_uri": "ws://35.131.123.111:4444/ws/v1/lambdas/connectback", # Websocket callback URL with API server direct IP
		}
		"""
		if "live_debug" in lambda_input[ "_refinery" ]:
			live_debug = lambda_input[ "_refinery" ][ "live_debug" ]
			live_debug[ "websocket" ] = create_connection(
				live_debug[ "websocket_uri" ]
			)
		
		# Check if it's just a warmup request
		if "warmup" in lambda_input[ "_refinery" ]:
			decrement_counter = lambda_input[ "_refinery" ][ "warmup" ]
			_warmup_concurrency_self_invoke(
				context.invoked_function_arn,
				decrement_counter
			)
			print( "Lambda warming event received, quitting out." )
			sys.stdout.flush()
			
			custom_runtime.send_response(
				request_id,
				""
			)
			return
		
		# Check if it's a temporary execution
		if "temporary_execution" in lambda_input[ "_refinery" ]:
			is_temporary_execution = True
		
		# Check if it's an SQS queue worker
		if "sqs_worker" in lambda_input[ "_refinery" ]:
			_sqs_worker(
				lambda_input[ "_refinery" ][ "sqs_worker" ][ "queue_insert_id" ],
				lambda_input[ "_refinery" ][ "sqs_worker" ][ "execution_id" ],
				lambda_input[ "_refinery" ][ "sqs_worker" ][ "branch_ids" ],
				lambda_input[ "_refinery" ][ "sqs_worker" ][ "fan_out_ids" ],
				context
			)
			custom_runtime.send_response(
				request_id,
				""
			)
			return
		
		# Set execution ID if set
		if "execution_id" in lambda_input[ "_refinery" ]:
			execution_id = lambda_input[ "_refinery" ][ "execution_id" ]

		# Set/propogate fan-out ID list if set
		if "fan_out_ids" in lambda_input[ "_refinery" ]:
			fan_out_ids = lambda_input[ "_refinery" ][ "fan_out_ids" ]

		# Throw exceptions fully (for use in tmp-runs, etc)
		if "throw_exceptions_fully" in lambda_input[ "_refinery" ]:
			throw_exceptions_fully = True

		# Set is invoke status
		is_spawner_invocation = False

		# We don't immediately invoke ourselves until after loading
		# useless return data to ensure idempotency
		if "invoke" in lambda_input[ "_refinery" ]:
			is_spawner_invocation = True
			invocation_id = lambda_input[ "_refinery" ][ "invoke" ]
			
		# This is for non-redis calls, e.g. through SNS or SQS
		if "backpack" in lambda_input[ "_refinery" ]:
			backpack = lambda_input[ "_refinery" ][ "backpack" ]

		_side_loaded = False
		if "indirect" in lambda_input[ "_refinery" ] and lambda_input[ "_refinery" ][ "indirect" ] and "type" in lambda_input[ "_refinery" ][ "indirect" ]:
			print( "Indirect load..." )
			sys.stdout.flush()

			# Input data is stored in redis
			if lambda_input[ "_refinery" ][ "indirect" ][ "type" ] == "redis":
				try:
					print( "Pulling data out of redis..." )
					sys.stdout.flush()
					stored_input = gmemory._get_input_data_from_redis(
						lambda_input[ "_refinery" ][ "indirect" ][ "key" ]
					)
					lambda_input = stored_input[ "input_data" ]
					backpack = stored_input[ "backpack" ]
				except AlreadyInvokedException as e:
					print( "This Lambda has already been invoked (or the return data has expired). For this reason we are quitting out (indirect return data)." )
					custom_runtime.send_response(
						request_id,
						""
					)
					return
				_side_loaded = True
			# Input data is stored in redis as a list from a fan-in
			elif lambda_input[ "_refinery" ][ "indirect" ][ "type" ] == "redis_fan_in":
				try:
					stored_input = gmemory._fan_in_get_results_data(
						lambda_input[ "_refinery" ][ "indirect" ][ "key" ]
					)
					lambda_input = stored_input[ "input_data" ]
					backpack = stored_input[ "backpack" ]
				except AlreadyInvokedException as e:
					print( "This Lambda has already been invoked (or the return data has expired). For this reason we are quitting out (direct return data)." )
					custom_runtime.send_response(
						request_id,
						""
					)
					return
				_side_loaded = True

		# Input data is stored in S3 and needs to be pulled
		# and de-serialized from JSON. Note that Redis is still used
		# to achieve idempotency so this will be called after the details
		# are first pulled from redis.
		# This structure is nested inside the regular redis storage
		print( "Input data before S3 struct scan: ")
		print(lambda_input)
		sys.stdout.flush()
		if is_external_s3_storage_structure( lambda_input ):
			print( "External data in S3, retrieving it..." )
			sys.stdout.flush()

			# Pull input data from S3
			stored_input = get_json_from_s3(
				lambda_input[ "_refinery" ][ "indirect" ][ "s3_bucket" ],
				lambda_input[ "_refinery" ][ "indirect" ][ "s3_path" ]
			)

			print( "Retrieved data:" )
			_pprint( stored_input )
			sys.stdout.flush()

			lambda_input = stored_input[ "input_data" ]
			backpack = stored_input[ "backpack" ]

			_side_loaded = True

		# If there's an "invoke" key it means we're doing a self-invocation fan-out
		if is_spawner_invocation:
			_spawner(
				branch_ids,
				invocation_id,
				context
			)
			custom_runtime.send_response(
				request_id,
				""
			)
			return
		# Just directly passed input data
		if not _side_loaded and "input_data" in lambda_input[ "_refinery" ]:
			lambda_input = lambda_input[ "_refinery" ][ "input_data" ]

	_stop_clock( "Loading Input Data" )

	# If we don't have an execution ID, generate and set one!
	if not execution_id:
		execution_id = str( uuid.uuid4() )

	# Set execution ID on Lambda context
	# This is needed so it can be accessed inside of the Lambdas
	# regular code.
	setattr(
		context,
		"execution_id",
		execution_id
	)

	return_data = {}

	if execution_mode == "REGULAR":
		_start_clock( "Executing Lambda" )

		process_handler = subprocess.Popen(
			[
				os.path.dirname(os.path.realpath(__file__)) + "/runtime",
				executable_path
			],
			stdout=subprocess.PIPE,
			stderr=subprocess.STDOUT,
			stdin=subprocess.PIPE,
			shell=False,
			universal_newlines=True,
			cwd=os.getenv( "LAMBDA_TASK_ROOT" ) + "/",
		)
		
		# Write input data
		process_handler.stdin.write(json.dumps({
			"lambda_input": lambda_input,
			"backpack": backpack
		}))
		process_handler.stdin.close()

		return_data = ""
		
		for line in iter( process_handler.stdout.readline, "" ):
			# If we're live streaming the output data, then pump it into
			# the WebSocket connection
			if live_debug and not line.startswith( "<REFINERY_" ):
				_stream_execution_output_to_websocket(
					live_debug[ "websocket" ],
					live_debug[ "debug_id" ],
					line
				)
			
			return_data += line
			
		while process_handler.returncode is None:
			process_handler.poll()
			
		return_data = return_data.strip()
		
		# Close the WebSocket connection now that we're finished
		if live_debug:
			live_debug[ "websocket" ].close()
		
		# If it's an inline execution we need to delete the file we just
		# executed so that it doesn't fill up the disk each run.
		if is_inline_execution:
			_clear_temporary_files()

		_stop_clock( "Executing Lambda" )

		if process_handler.returncode == 0:
			# We now look for the markers to indicate data is being returned.
			start_marker = "<REFINERY_OUTPUT_CUSTOM_RUNTIME_START_MARKER>"
			end_marker = "<REFINERY_OUTPUT_CUSTOM_RUNTIME_END_MARKER>"
			program_output = ""

			# If we have the marker, pull it out.
			if start_marker in return_data and end_marker in return_data:
				return_data_parts = return_data.split( start_marker )
				program_output = return_data_parts[0]
				return_data_sub_parts = return_data_parts[1].split( end_marker )
				return_data_string = return_data_sub_parts[0]
				try:
					return_data = json.loads(
						return_data_string
					)
					backpack = return_data[ "backpack" ]
					return_data = return_data[ "output" ]
				except:
					return_data = return_data_string
					
				# Write the output for logging
				sys.stdout.write( program_output )
				sys.stdout.flush()
			else:
				# Write the output for logging
				sys.stdout.write( return_data )
				sys.stdout.flush()
				# If there's no marker we're not returning anything.
				# Just return nothing
				return_data = ""
		else:
			error_output = return_data

			error_start_marker = "<REFINERY_ERROR_OUTPUT_CUSTOM_RUNTIME_START_MARKER>"
			error_end_marker = "<REFINERY_ERROR_OUTPUT_CUSTOM_RUNTIME_END_MARKER>"

			if error_start_marker in str( error_output ) and error_end_marker in str( error_output ):
				exception_parts = error_output.split( error_start_marker )
				error_output = exception_parts[0]
				exception_data_sub_parts = exception_parts[1].split( error_end_marker )
				exception_string = exception_data_sub_parts[0]
			else:
				exception_string = ""
				
			try:
				output_data = json.loads(
					exception_string
				)
				exception_string = output_data[ "output" ]
				backpack = output_data[ "backpack" ]
				error_output = error_output + exception_string
			except:
				# We pass here since the result is the same
				# Just an unmodified backpack
				pass

			_stop_clock( "Executing Lambda" )
			
			invocation_input_list = []

			# Invoke all Lambdas to be run when an exception occurs
			if len( transitions[ "exception" ] ) > 0:
				if execution_pipeline_logging_level == "LOG_ERRORS" or execution_pipeline_logging_level == "LOG_ALL":
					_write_pipeline_logs(
						s3_log_bucket,
						execution_pipeline_id,
						context.invoked_function_arn,
						context.function_name,
						execution_id,
						"CAUGHT_EXCEPTION",
						execution_details,
						error_output,
						backpack,
						lambda_input,
						""
					)

				for exception_transition_data in transitions[ "exception" ]:
					invocation_input_list.append({
						"execution_id": execution_id,
						"fan_out_ids": fan_out_ids,
						"type": exception_transition_data[ "type" ],
						"arn": exception_transition_data[ "arn" ],
						"backpack": backpack,
						"input_data": {
							"version": _VERSION,
							"exception_text": exception_string,
							"input_data": json.loads(
								json.dumps(
									lambda_input
								)
							)
						}
					})
			elif execution_pipeline_logging_level == "LOG_ERRORS" or execution_pipeline_logging_level == "LOG_ALL":
				_write_pipeline_logs(
					s3_log_bucket,
					execution_pipeline_id,
					context.invoked_function_arn,
					context.function_name,
					execution_id,
					"EXCEPTION",
					execution_details,
					error_output,
					backpack,
					lambda_input,
					""
				)
			
			# If we've got an API Gateway response coming up we'll write an error
			# to it so that it's more obvious what's happening.
			if _transition_type_in_transitions( transitions, "api_gateway_response" ):
				invocation_input_list.append({
					"backpack": backpack,
					"execution_id": execution_id,
					"type": "api_gateway_response",
					"input_data": {
						"success": False,
						"msg": "An exception occurred while computing the response to the request. Please see the block execution logs for more information."
					}
				})
				
			# If there's an exception we should stop after invoking the exception case(s).
			_parallel_invoke( branch_ids, invocation_input_list )
			
			# Write error to stderr for logging
			sys.stderr.write( error_output )
			sys.stderr.flush()
			
			return_data = ""
			
			# If we're doing a temporary execution we want to format the returned
			# data in the special format for the API server to pull the full data out
			# of S3.
			if is_temporary_execution:
				execution_result_details = _write_temporary_execution_results(
					s3_log_bucket,
					error_output,
					""
				)
				
				return_data = {
					"_refinery": {
						"indirect": {
							"type": "s3",
							"s3_bucket": execution_result_details[ "s3_bucket" ],
							"s3_path": execution_result_details[ "s3_path" ]
						}
					}
				}
			
			# We only want to do this if we've been asked to fully throw exceptions
			if throw_exceptions_fully:
				custom_runtime.send_error(
					request_id,
					return_data, # Just blank output for errors
				)
				return
			
			custom_runtime.send_response(
				request_id,
				return_data
			)
			return
		
		# Attempt to convert return_data into an actual
		# object if it's JSON
		try:
			return_data = json.loads(
				return_data
			)
		except:
			pass

	elif execution_mode == "API_ENDPOINT":
		# Just return the HTTP request event data
		return_data = lambda_input
		
	# If it's a temporary execution we can stop here
	# and just immediately return our special format
	if is_temporary_execution:
		execution_result_details = _write_temporary_execution_results(
			s3_log_bucket,
			program_output,
			return_data
		)
		
		return_data = {
			"_refinery": {
				"indirect": {
					"type": "s3",
					"s3_bucket": execution_result_details[ "s3_bucket" ],
					"s3_path": execution_result_details[ "s3_path" ]
				}
			}
		}
		
		custom_runtime.send_response(
			request_id,
			return_data
		)
		
		return
	
	# If it's an API gateway Lambda we can end it here.
	if execution_mode == "API_ENDPOINT":
		# Used for formatting AWS API Gateway data to be more
		# usable for regular users.
		try:
			lambda_input = _improve_api_endpoint_request_data(
				lambda_input
			)
		except Exception as e:
			print( "[ Refinery Custom Runtime ] An exception occurred while decoding the request data." )
			print( e )
			sys.stdout.flush()

	# Stores the invocation data for "if"s and "then"s
	invocation_input_list = []

	# For fan-outs
	for fan_out_transition_data in transitions[ "fan-out" ]:
		if type( return_data ) != list:
			# Write error to stderr for logging
			sys.stderr.write(
				"Error, tried to fan-out without returning a list, returned type " + str( type( return_data ) )
			)
			sys.stderr.flush()
			custom_runtime.send_response(
				request_id,
				"" # Just blank output for errors
			)
			return
		
		# If there's an empty list returned from a block in a fan-out
		# we can just skip the next transition.
		if len( return_data ) == 0:
			continue

		# Generate fan-out ID
		fan_out_id = str( uuid.uuid4() )

		# Add to our fan-out IDs
		new_fan_out_ids = fan_out_ids
		new_fan_out_ids.append(
			fan_out_id
		)

		# Create a list of invocation inputs
		invocation_list = []

		# Iterate over each item in the return list
		for return_item_data in return_data:
			# Create an invocation for it
			# Convert into JSON so it can be stored in redis
			invocation_list.append(
				{
					"backpack": backpack,
					"execution_id": execution_id,
					"fan_out_ids": new_fan_out_ids,
					"type": "fan_out_execution",
					"arn": fan_out_transition_data[ "arn" ],
					"input_data": json.loads(
						json.dumps(
							return_item_data
						)
					)
				}
			)

		# Convert all big return data values to S3 refs
		invocation_list = convert_invoke_data_list_into_s3_extended(
			invocation_list
		)

		print( "S3 converted invocation list: " )
		print( json.dumps(invocation_list) )
		sys.stdout.flush()

		# Encode all data
		for i in range( 0, len( invocation_list ) ):
			invocation_list[i] = json.dumps(
				invocation_list[i]
			)

		print( "Encoded invocation list: " )
		print( json.dumps(invocation_list) )
		sys.stdout.flush()

		# Sets up atomic counter for fan-in
		invocation_id = gmemory._set_fan_in_data(
			fan_out_id,
			invocation_list
		)

		"""
		Calculate the fan-out invocation speed (A.K.A. the
		number of spawners we'll spin up to invoke it all).
		
		There are three factors for this:
		* Remaining execution time (to invoke the spawners).
		* Execution memory (affects how invocating can be done)
		* Number of invocations to perform
		
		The remaining execution time is used as the ceiling
		and the number of invocations to perform is used to
		figure out the velocity.
		"""
		total_lambda_execution_time = int( math.ceil( time.time() - start_time ) ) + int( context.get_remaining_time_in_millis() / 1000 )
		remaining_seconds = int(
			math.floor(
				( context.get_remaining_time_in_millis() / 1000 )
			)
		)

		# Conservative estimates of how many lambdas can
		# be executed per second for each memory range.
		# These are pretty low balls but better safe than sorry!
		if int( context.memory_limit_in_mb ) <= 256:
			lambdas_per_second = 2
		elif int( context.memory_limit_in_mb ) <= 576:
			lambdas_per_second = 10
		elif int( context.memory_limit_in_mb ) > 576:
			lambdas_per_second = 13

		# Calculate max number of Lambdas we //could// execute
		max_lambda_invocations_possible = ( lambdas_per_second * remaining_seconds )

		# Number of invocations we actually have to execute
		number_of_executions_to_perform = len( invocation_list )

		# Number of invocations a fresh-started Lambda could do
		# Minus one for the initialization cost that may be incurred
		total_lambda_executions_per_run = ( lambdas_per_second * ( total_lambda_execution_time - 2 ) )

		# Calculate the number of full runs it'd take to process all the invocations
		number_of_runs_to_complete = int( math.ceil( float( number_of_executions_to_perform ) / float( total_lambda_executions_per_run ) ) )

		# Now calculate the number of spawners to kick off.
		if number_of_runs_to_complete <= max_lambda_invocations_possible:
			# If the number of runs to complete the work is less than the
			# number of invocations possible we'll just invoke that many
			# spawner Lambda(s)!
			fan_out_invocation_speed = number_of_runs_to_complete
		else:
			# Otherwise we'll just invoke the max possible
			fan_out_invocation_speed = max_lambda_invocations_possible


		def get_seconds_to_completion( number_of_executions_to_perform, lambdas_per_second, fan_out_invocation_speed ):
			return ( float( number_of_executions_to_perform ) / float( lambdas_per_second * fan_out_invocation_speed ) )

		seconds_to_complete = get_seconds_to_completion(
			number_of_executions_to_perform,
			lambdas_per_second,
			fan_out_invocation_speed
		)

		# This is the target seconds we shoot for to spawn everything.
		target_seconds = 10

		# Keep boosting
		while seconds_to_complete > target_seconds:
			seconds_to_complete = get_seconds_to_completion(
				number_of_executions_to_perform,
				lambdas_per_second,
				( fan_out_invocation_speed + 1 )
			)

			# If boosting would push us over max Lambda invocations quit out
			if ( fan_out_invocation_speed + 1 ) > max_lambda_invocations_possible:
				break

			# Meets criteria so we can up it
			fan_out_invocation_speed += 1

			# If we meet our target time then we can stop
			if seconds_to_complete <= target_seconds:
				break

		seconds_to_complete = get_seconds_to_completion(
			number_of_executions_to_perform,
			lambdas_per_second,
			fan_out_invocation_speed
		)

		# Invoke self in "spawner" mode X times
		# where X is invocation speed
		for i in range( 0, fan_out_invocation_speed ):
			invocation_input_list.append({
				"type": "lambda_fan_out",
				"arn": context.invoked_function_arn,
				"invocation_id": invocation_id,
				"backpack": backpack,
				# Input data is purely to ensure we get idempotency
				"input_data": {
					"why": "idempotency"
				}
			})

	# For fan-in
	for fan_in_transition_data in transitions[ "fan-in" ]:
		# If they do a fan-in and they haven't done a fan-out
		if len( fan_out_ids ) == 0:
			break
		
		# Get last fan-out ID
		fan_out_id = fan_out_ids.pop()

		# First we push our returned data into the results list
		fan_in_results = gmemory._fan_in_operations(
			fan_out_id,
			return_data,
			backpack
		)

		# If we're the final invocation in the fan-in we
		# will invoke the next Lambda with the return data
		# array as the input.
		if fan_in_results:
			invocation_input_list.append({
				"backpack": backpack,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": "lambda_fan_in",
				"arn": fan_in_transition_data[ "arn" ],
				"fan_out_id": fan_out_id
			})

	# If it's just a then, just invoke the next Lambda
	for then_transition_data in transitions[ "then" ]:
		invocation_input_list.append({
			"backpack": backpack,
			"execution_id": execution_id,
			"fan_out_ids": fan_out_ids,
			"type": then_transition_data[ "type" ],
			"arn": then_transition_data[ "arn" ],
			"context": context,
			"input_data": json.loads(
				json.dumps(
					return_data
				)
			)
		})

	# For merge transitions
	for merge_transition_data in transitions[ "merge" ]:
		invocation_input_list.append({
			"invoked_function_arn": context.invoked_function_arn,
			"merge_lambdas": merge_transition_data[ "merge_lambdas" ],
			"backpack": backpack,
			"execution_id": execution_id,
			"fan_out_ids": fan_out_ids,
			"type": "merge",
			"arn": merge_transition_data[ "arn" ],
			"input_data": json.loads(
				json.dumps(
					return_data
				)
			)
		})

	# If it's an API gateway Lambda we can end it here.
	if execution_mode == "API_ENDPOINT":
		# Now we invoke all the queued Lambdas!
		_parallel_invoke( branch_ids, invocation_input_list )

		custom_runtime.send_response(
			request_id,
			_api_endpoint(
				lambda_input,
				execution_id,
				context
			)
		)
		return

	# Variable to hold if any "if" statements evaluated to true
	# if one has then we don't execute any "else" statements
	true_if_evaluation_occured = False

	# Iterate over every if
	for if_statement_data in transitions[ "if" ]:
		try:
			expression_eval_result = eval( if_statement_data[ "expression" ] )
		except Exception, err:
			exception_string = "Your \"if\" transition's conditional logic code has thrown an exception!\n"
			exception_string += "Please note it must be valid Python 2.7 code. The exception is the following: \n"
			exception_string += traceback.format_exc()
			print( exception_string )
			sys.stdout.flush()
			custom_runtime.send_error(
				request_id,
				return_data, # Just blank output for errors
			)
			_write_pipeline_logs(
				os.environ[ "LOG_BUCKET_NAME" ],
				os.environ[ "EXECUTION_PIPELINE_ID" ],
				context.invoked_function_arn,
				context.function_name,
				execution_id,
				"EXCEPTION",
				execution_details,
				exception_string,
				{},
				lambda_input,
				""
			)
			return

		if expression_eval_result:
			invocation_input_list.append({
				"backpack": backpack,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": if_statement_data[ "type" ],
				"arn": if_statement_data[ "arn" ],
				"context": context,
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})

			true_if_evaluation_occured = True

	# If else is set, call that now
	if true_if_evaluation_occured == False:
		for else_transition_data in transitions[ "else" ]:
			invocation_input_list.append({
				"backpack": backpack,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": else_transition_data[ "type" ],
				"arn": else_transition_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})

	# Now we invoke all the queued Lambdas!
	_parallel_invoke( branch_ids, invocation_input_list )

	# If pipeline logging is enabled
	# Write the return data of this Lambda
	if execution_pipeline_logging_level == "LOG_ALL":
		_write_pipeline_logs(
			s3_log_bucket,
			execution_pipeline_id,
			context.invoked_function_arn,
			context.function_name,
			execution_id,
			"SUCCESS",
			execution_details,
			program_output,
			backpack,
			lambda_input,
			return_data
		)

	# Return the return data as usual
	custom_runtime.send_response(
		request_id,
		json.dumps(
			return_data
		)
	)
	return

"""
{
	"action_name": {
		"start": time.time(),
		"end": time.time(),
		"total": 0,
	}
}
"""
_CLOCK_TIMES = {}
def _start_clock( action_name ):
	return
	global _CLOCK_TIMES
	_CLOCK_TIMES[ action_name ] = {
		"start": datetime.datetime.now(),
		"end": 0,
		"total": 0,
	}

def _stop_clock( action_name ):
	return
	global _CLOCK_TIMES
	_CLOCK_TIMES[ action_name ][ "end" ] = datetime.datetime.now()
	_CLOCK_TIMES[ action_name ][ "total" ] = (
			(
					time.mktime(_CLOCK_TIMES[ action_name ][ "end" ].timetuple())*1e3 + _CLOCK_TIMES[ action_name ][ "end" ].microsecond/1e3
			) - (
					time.mktime(_CLOCK_TIMES[ action_name ][ "start" ].timetuple())*1e3 + _CLOCK_TIMES[ action_name ][ "start" ].microsecond/1e3
			)
	)
	print( "[ INFO ] The action '" + action_name + "' took " + str( ( _CLOCK_TIMES[ action_name ][ "total" ] * 0.001 ) ) + " second(s)." )

class CustomRuntime():
	"""
	This custom runtime will run a given script with a custom interpreter.

	The executed script is expected to return data in the following format:

	{
		"output": "Running script...\nDone!",
		"return_data": "123",
	}
	"""
	def __init__( self ):
		runtime_endpoint = os.getenv( "AWS_LAMBDA_RUNTIME_API" )
		handler = os.getenv( "_HANDLER" )
		handler_parts = handler.split( "." )
		handler_file = handler_parts[0]
		handler_function = handler_parts[1]
		self.base_invocation_uri = "http://" + runtime_endpoint + "/2018-06-01/runtime/invocation"

	def process_next_event( self ):
		event_data = self.get_next_invocation()

		context_dict = {
			"function_name": os.getenv( "AWS_LAMBDA_FUNCTION_NAME" ),
			"function_version": os.getenv( "AWS_LAMBDA_FUNCTION_VERSION" ),
			"invoked_function_arn": event_data[ "invoked_arn" ],
			"memory_limit_in_mb": os.getenv( "AWS_LAMBDA_FUNCTION_MEMORY_SIZE" ),
			"aws_request_id": event_data[ "request_id" ],
			"log_group_name": os.getenv( "AWS_LAMBDA_LOG_GROUP_NAME" ),
			"log_stream_name": os.getenv( "AWS_LAMBDA_LOG_STREAM_NAME" ),
			"deadline_ms": event_data[ "deadline_ms" ]
		}

		# Create a context object
		class ContextObject():
			def __init__( self, context_dict ):
				# Set values of context object to context dict
				for key, value in context_dict.iteritems():
					setattr( self, key, value )

				# Set inner dict
				self.context_dict = context_dict

			def get_remaining_time_in_millis( self ):
				return self.deadline_ms - int( time.time() * 1000 )

		_start_clock( "Context Object Initialization" )
		new_context = ContextObject( context_dict )
		_stop_clock( "Context Object Initialization" )

		lambda_input = event_data[ "event_data" ]

		try:
			lambda_input = json.loads(
				lambda_input
			)
		except:
			pass
	
		# Construct log details for debugging
		execution_details = {
			"initialization_time": int( time.time() ),
			"aws_region": os.environ[ "AWS_REGION" ],
			"group_name": new_context.log_group_name,
			"stream_name": new_context.log_stream_name,
			"function_name": new_context.function_name,
			"function_version": new_context.function_version,
			"invoked_function_arn": new_context.invoked_function_arn,
			"memory_limit_in_mb": int( new_context.memory_limit_in_mb ),
			"aws_request_id": new_context.aws_request_id
		}

		# Run through the custom Refinery runtime.
		try:
			return _init(
				self,
				event_data[ "request_id" ],
				lambda_input,
				new_context,
				execution_details
			)
		except Exception, err:
			exception_string = "You've found a bug in the Refinery custom runtime!\n"
			exception_string += "Please report the following exception to us: \n"
			exception_string += traceback.format_exc()
			print( exception_string )
			sys.stdout.flush()
			
			_write_pipeline_logs(
				os.environ[ "LOG_BUCKET_NAME" ],
				os.environ[ "EXECUTION_PIPELINE_ID" ],
				new_context.invoked_function_arn,
				new_context.function_name,
				lambda_input[ "_refinery" ][ "execution_id" ],
				"EXCEPTION",
				execution_details,
				exception_string,
				{},
				lambda_input,
				""
			)
			
			# Return successful state
			self.send_response(
				event_data[ "request_id" ],
				""
			)
			return

	def get_next_invocation( self ):
		response = http.request(
			"GET",
			self.base_invocation_uri + "/next"
		)

		return {
			"deadline_ms": int( response.headers[ "Lambda-Runtime-Deadline-Ms" ] ),
			"invoked_arn": response.headers[ "Lambda-Runtime-Invoked-Function-Arn" ],
			"request_id": response.headers[ "Lambda-Runtime-Aws-Request-Id" ],
			"event_data": response.data
		}

	def send_response( self, request_id, return_data ):
		response = http.request(
			"POST",
			self.base_invocation_uri + "/" + request_id + "/response",
			body=json.dumps(
				return_data
			),
			headers={
				"Content-Type": "application/json",
			}
		)

	def send_error( self, request_id, error_data ):
		response = http.request(
			"POST",
			self.base_invocation_uri + "/" + request_id + "/error",
			body=json.dumps(
				error_data
			),
			headers={
				"Content-Type": "application/json",
			}
		)

# We'll be called by the AWS Custom Runtime agent
if __name__ == "__main__":
	new_runtime = CustomRuntime()

	# Loop infinitely to keep processing events
	while True:
		new_runtime.process_next_event()