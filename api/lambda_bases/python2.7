_VERSION = "1.0.0"

"""
Base redis connection for pulling configs/etc.

Allows for single-millisecond retrieval and setting
of data in the main redis memory cache.
"""
import os
import uuid
import json
import time
import redis
import pickle
import boto3
import traceback
import threading

# TODO only initialize connection when used.
class Refinery_Memory:
	# 16 minutes default return data expire
	return_data_timeout = ( 60 * 16 )
	
	json_types = [
		list,
		dict
	]
	
	regular_types = [
		str,
		int,
		float,
		complex,
		bool,
	]
	
	def __init__( self, in_hostname, in_password, namespace ):
		self.redis_client = False
		self.namespace = namespace
		self.hostname = in_hostname
		self.password = in_password
		
	def connect( self ):
		self.redis_client = redis.StrictRedis(
			host=self.hostname,
			port=6379,
			db=0,
			socket_timeout=2,
			password=self.password,
		)
		
	def _get_namespace( self, kwargs ):
		if self.namespace == False:
			return ""

		if "raw" in kwargs and kwargs[ "raw" ]:
			return ""
			
		return self.namespace + "."
	
	def set( self, key, input_data, **kwargs ):
		if not self.redis_client:
			self.connect()
		
		namespace = self._get_namespace( kwargs )
			
		if type( input_data ) in self.regular_types:
			self.redis_client.set(
				namespace + key,
				input_data
			)
		elif type( input_data ) in self.json_types:
			self.redis_client.set(
				namespace + key,
				json.dumps(
					input_data
				)
			)
		else:
			self.redis_client.set(
				namespace + key,
				pickle.dumps(
					input_data
				)
			)
			
	def get( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		data = self.redis_client.get(
			namespace + key
		)
		
		try:
			return json.loads(
				data
			)
		except:
			pass
		
		try:
			return pickle.loads(
				data
			)
		except:
			pass
		
		return data
			
	def exists( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.exists(
			namespace + key
		)
		
	def delete( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.dek(
			namespace + key
		)
		
	def rename( self, key, new_key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.rename(
			namespace + key,
			new_key
		)
		
	def expire_at( self, key, unix_timestamp, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.expireat(
			namespace + key,
			unix_timestamp
		)
		
	def expire_in( self, key, seconds, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.expire(
			namespace + key,
			seconds
		)
		
	def _get_input_data_from_redis( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		pipeline = self.redis_client.pipeline()
		pipeline.get( key )
		pipeline.delete( key )
		returned_data = pipeline.execute()
		
		try:
			return json.loads( returned_data[ 0 ] )
		except:
			pass
		
		return returned_data[ 0 ]
		
	def _store_return_data_to_redis( self, return_data, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		new_key = str( uuid.uuid4() )
			
		self.redis_client.setex(
			new_key,
			self.return_data_timeout,
			json.dumps(
				return_data
			)
		)
		
		return new_key
		
def _parallel_invoke( invoke_queue ):
	# List of active threads
	active_threads = []
	# Maximum number of threads
	max_threads = 10
	
	# Invokes a Lambda asynchronously
	def lambda_invoker_worker( execution_id, arn, input_data ):
		storage_key = gmemory._store_return_data_to_redis( input_data )
		
		return_wrapper_data = {
			"_refinery": {
				"indirect": {
					"type": "redis",
					"key": storage_key,
				},
				"parallel": False,
				"execution_id": execution_id,
			}
		}
		
		lambda_client = boto3.client(
			"lambda"
		)
	
		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="RequestResponse",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)
		
		raise SystemExit
		
	# Publishes to an SNS topic
	def sns_topic_publish_worker( execution_id, arn, input_data ):
		# Handle large data that won't fit in the 256K SNS max size
		sns_client = boto3.client(
			"sns"
		)
		
		return_wrapper_data = {
			"_refinery": {
				"indirect": False,
				"parallel": False,
				"execution_id": execution_id,
				"input_data": input_data,
			}
		}
	
		response = sns_client.publish(
			TopicArn=arn,
			Message=json.dumps(
				return_wrapper_data
			)
		)
		
		raise SystemExit
		
	# Publishes to redis to be picked up by a polling API Gateway Lambda
	def api_gateway_response( execution_id, arn, input_data ):
		# Store with expiration in redis
		gmemory.redis_client.setex(
			execution_id,
			gmemory.return_data_timeout,
			json.dumps(
				input_data
			)
		)
		
		raise SystemExit
	
	# Spawns a new worked and adds it to active_threads
	def spawn_new_worker():
		new_invoke_data = invoke_queue.pop()
		
		# Check type and invoke appropriately
		if new_invoke_data[ "type" ] == "lambda":
			new_thread = threading.Thread(
				target=lambda_invoker_worker,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "sns_topic":
			new_thread = threading.Thread(
				target=sns_topic_publish_worker,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "api_gateway_response":
			new_thread = threading.Thread(
				target=api_gateway_response,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()

		return new_thread
		
	# Bug fix test
	lambda_client = boto3.client(
		"lambda"
	)
	
	# Keep looping while there's still Lambdas to invoke
	while len( invoke_queue ) > 0:
		# If we've not maxing out acive threads then spawn new ones
		if len( active_threads ) < max_threads:
			active_threads.append(
				spawn_new_worker()
			)
		else:
			# Iterate over our active threads and remove finished
			new_active_thread_list = []
			already_spawned = False
			# Check if any threads are finished
			for active_thread in active_threads:
				if active_thread.is_alive():
					new_active_thread_list.append( active_thread )
				elif already_spawned == False:
					new_active_thread_list.append(
						spawn_new_worker()
					)
					already_spawned = True
					
			active_threads = new_active_thread_list
			
	# Wait until all threads finish
	for thread in active_threads:
		thread.join()
	
	# We're all done
	return

def _api_endpoint( execution_id, context ):
    # This will spin and continually query redis until either
    # we time out without getting our HTTP response OR we return
    # our response to the client.
    
    # Continually loop until we have only two seconds left
    # Max execution time is 30 seconds, so that's 28 seconds
    timed_out = True

    # As long as we have ~2 seconds of runway left continue
    # to query redis for our HTTP response data.
    while context.get_remaining_time_in_millis() > ( 2 * 1000 ):
        http_response = gmemory._get_input_data_from_redis(
            execution_id
        )
        
        # When we have a non-None http_response we can
        # break out of the loop and declare we've not timed out.
        if http_response:
            timed_out = False
            break
        
        # Wait a moment before checking again
        time.sleep( 0.01 )
    
    # We've timed out, return an error
    if timed_out:
        return {
        	"statusCode": 504,
        	"headers": {},
        	"body": json.dumps({
        	    "msg": "The request to the backend has timed out.",
        	    "success": False
        	}),
        	"isBase64Encoded": False
        }
    
    # Check if the response is actually in an API Gateway already
    # If not return as just regular JSON, if it is then return raw
    if "body" in http_response:
        return http_response
    
    # Return JSON response with the data
    return {
    	"statusCode": 200,
    	"headers": {
    		"Content-Type": "application/json",
    		"X-Frame-Options": "deny",
    		"X-Content-Type-Options": "nosniff",
    		"X-XSS-Protection": "1; mode=block",
    		"Cache-Control": "no-cache, no-store, must-revalidate",
    		"Pragma": "no-cache",
    		"Expires": "0",
    		"Server": "refinery"
    	},
    	"body": json.dumps( http_response ),
    	"isBase64Encoded": False
    }

def _init( lambda_input, context ):
	# TODO add timer
	# Bake in AWS region
	aws_region = "{{AWS_REGION_REPLACE_ME}}"
	
	# Indicate a special execution condition
	# For example, an API Gateway Lambda
	execution_mode = "{{SPECIAL_EXECUTION_MODE}}"
	
	# Construct log details for debugging
	log_details = {
		"initialization_time": int( time.time() ),
		"aws_region": aws_region,
		"group_name": context.log_group_name,
		"stream_name": context.log_stream_name,
		"function_name": context.function_name,
		"function_version": context.function_version,
		"invoked_function_arn": context.invoked_function_arn,
		"memory_limit_in_mb": context.memory_limit_in_mb,
		"aws_request_id": context.aws_request_id
	}
	
	global cmemory
	global gmemory
	
	cmemory = Refinery_Memory(
		"config-memory.refinery.thehackerblog.com",
		"{{REDIS_PASSWORD_REPLACE_ME}}",
		False
	)
	
	gmemory = Refinery_Memory(
		"global-memory.refinery.thehackerblog.com",
		"{{REDIS_PASSWORD_REPLACE_ME}}",
		context.function_name
	)
	
	# Execution ID, this is an ID which correlates to an execution chain
	execution_id = False
	
	# Bake in transition data
	transitions = json.loads( "{{TRANSITION_DATA_REPLACE_ME}}" )
	
	# Detect refinery wrapper and unwrap if existant
	# Else just leave it unmodified
	if lambda_input and "_refinery" in lambda_input:
		# Set execution ID if set
		if "execution_id" in lambda_input[ "_refinery" ]:
			execution_id = lambda_input[ "_refinery" ][ "execution_id" ]
			
		_side_loaded = False
		if "indirect" in lambda_input[ "_refinery" ] and "type" in lambda_input[ "_refinery" ][ "indirect" ]:
			# Input data is stored in redis
			if lambda_input[ "_refinery" ][ "indirect" ][ "type" ] == "redis":
				lambda_input = gmemory._get_input_data_from_redis(
					lambda_input[ "_refinery" ][ "indirect" ][ "key" ]
				)
				_side_loaded = True
		
		if not _side_loaded and "input_data" in lambda_input[ "_refinery" ]:
			lambda_input = lambda_input[ "_refinery" ][ "input_data" ]
	
	# If we don't have an execution ID, generate and set one!
	if not execution_id:
		execution_id = str( uuid.uuid4() )
		
	# Set execution ID on Lambda context
	# This is needed so it can be accessed inside of the Lambdas
	# regular code.
	setattr(
		context,
		"execution_id",
		execution_id
	)
	
	return_data = {}
	
	if execution_mode == "REGULAR":
		# Catch any exception that occurs and handle it if a catch is defined.
		try:
			return_data = main( lambda_input, context )
		except:
			exception_string = str( traceback.format_exc() )
			
			# Invoke all Lambdas to be run when an exception occurs
			if len( transitions[ "exception" ] ) > 0:
				invocation_input_list = []
				for exception_transition_data in transitions[ "exception" ]:
					invocation_input_list.append({
						"execution_id": execution_id,
						"type": exception_transition_data[ "type" ],
						"arn": exception_transition_data[ "arn" ],
						"input_data": {
							"version": _VERSION,
							"exception_text": exception_string,
							"input_data": json.loads(
								json.dumps(
									lambda_input
								)
							)
						}
					})
				
				
				# If there's an exception we should stop after invoking the exception case(s).
				_parallel_invoke( invocation_input_list )
				raise
			else:
				# Nothing to handle it, throw it again.
				raise
	
	# Stores the invocation data for "if"s and "then"s
	invocation_input_list = []
	
	# If it's just a then, just invoke the next Lambda
	for then_transition_data in transitions[ "then" ]:
		invocation_input_list.append({
			"execution_id": execution_id,
			"type": then_transition_data[ "type" ],
			"arn": then_transition_data[ "arn" ],
			"input_data": json.loads(
				json.dumps(
					return_data
				)
			)
		})
	
	# If it's an API gateway Lambda we can end it here.
	if execution_mode == "API_ENDPOINT":
		# Now we invoke all the queued Lambdas!
		_parallel_invoke( invocation_input_list )
		
		return _api_endpoint( execution_id, context )
	
	# Variable to hold if any "if" statements evaluated to true
	# if one has then we don't execute any "else" statements
	true_if_evaluation_occured = False
	
	# Iterate over every if
	for if_statement_data in transitions[ "if" ]:
		expression_eval_result = eval( if_statement_data[ "expression" ] )
		
		if expression_eval_result:
			invocation_input_list.append({
				"execution_id": execution_id,
				"type": if_statement_data[ "type" ],
				"arn": if_statement_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})
			
			true_if_evaluation_occured = True
		
	# If else is set, call that now
	if true_if_evaluation_occured == False:
		for else_transition_data in transitions[ "else" ]:
			invocation_input_list.append({
				"execution_id": execution_id,
				"type": else_transition_data[ "type" ],
				"arn": else_transition_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})
	
	# Now we invoke all the queued Lambdas!
	_parallel_invoke( invocation_input_list )
	
	# Return the return data as usual
	return return_data
