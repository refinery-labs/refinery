_VERSION = "1.0.0"

"""
Base redis connection for pulling configs/etc.

Allows for single-millisecond retrieval and setting
of data in the main redis memory cache.
"""
import os
import uuid
import json
import time
import redis
import pickle
import boto3
import traceback
import threading

class AlreadyInvokedException( Exception ):
	def __init__( self, message="This function has already been invoked!" ):
		# Call the base class constructor with the parameters it needs
		super( AlreadyInvokedException, self ).__init__( message )

# TODO only initialize connection when used.
class Refinery_Memory:
	# 16 minutes default return data expire
	# TODO in the future count the number of nodes in between and set timeout to be max
	# time allocated for all the Lambdas in the path to run
	return_data_timeout = ( 60 * 16 )
	
	json_types = [
		list,
		dict
	]
	
	regular_types = [
		str,
		int,
		float,
		complex,
		bool,
	]
	
	def __init__( self, in_hostname, in_password, namespace ):
		self.redis_client = False
		self.namespace = namespace
		self.hostname = in_hostname
		self.password = in_password
		
	def connect( self ):
		self.redis_client = redis.StrictRedis(
			host=self.hostname,
			port=6379,
			db=0,
			socket_timeout=2,
			password=self.password,
		)
		
	def _get_namespace( self, kwargs ):
		if self.namespace == False:
			return ""

		if "raw" in kwargs and kwargs[ "raw" ]:
			return ""
			
		return self.namespace + "."
	
	def set( self, key, input_data, **kwargs ):
		if not self.redis_client:
			self.connect()
		
		namespace = self._get_namespace( kwargs )
			
		if type( input_data ) in self.regular_types:
			self.redis_client.set(
				namespace + key,
				input_data
			)
		elif type( input_data ) in self.json_types:
			self.redis_client.set(
				namespace + key,
				json.dumps(
					input_data
				)
			)
		else:
			self.redis_client.set(
				namespace + key,
				pickle.dumps(
					input_data
				)
			)
			
	def get( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		data = self.redis_client.get(
			namespace + key
		)
		
		try:
			return json.loads(
				data
			)
		except:
			pass
		
		try:
			return pickle.loads(
				data
			)
		except:
			pass
		
		return data
			
	def exists( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.exists(
			namespace + key
		)
		
	def delete( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.dek(
			namespace + key
		)
		
	def rename( self, key, new_key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.rename(
			namespace + key,
			new_key
		)
		
	def expire_at( self, key, unix_timestamp, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.expireat(
			namespace + key,
			unix_timestamp
		)
		
	def expire_in( self, key, seconds, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		namespace = self._get_namespace( kwargs )
		
		return self.redis_client.expire(
			namespace + key,
			seconds
		)
		
	def _get_input_data_from_redis( self, key, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		pipeline = self.redis_client.pipeline()
		pipeline.get( key )
		pipeline.delete( key )
		returned_data = pipeline.execute()
		
		# Raise an exception if the GET fails
		# This is how we achieve idempotency
		if returned_data[ 0 ] == None:
			raise AlreadyInvokedException()
		
		try:
			return json.loads( returned_data[ 0 ] )
		except:
			pass
		
		return returned_data[ 0 ]
		
	def _store_return_data_to_redis( self, return_data, **kwargs ):
		if not self.redis_client:
			self.connect()
			
		new_key = str( uuid.uuid4() )
			
		self.redis_client.setex(
			new_key,
			self.return_data_timeout,
			json.dumps(
				return_data
			)
		)
		
		return new_key
		
	def _set_fan_in_data( self, fan_out_id, fan_out_number, **kwargs ):
		if not self.redis_client:
			self.connect()
		
		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()
		pipeline.setex(
			"FAN_IN_COUNTER_" + fan_out_id,
			( 60 * 15 * 3 ), # TODO dynamically calculate based off of nodes in path worst-case
			fan_out_number
		)
		returned_data = pipeline.execute()
		
		return
	
	def _fan_in_get_results_data( self, fan_out_id, **kwargs ):
		"""
		Gets the fan-in data and deletes it immediately
		"""
		if not self.redis_client:
			self.connect()
			
		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()
		
		# Get array of returned data from fan-in
		pipeline.lrange(
			"FAN_IN_RESULTS_" + fan_out_id,
			0,
			-1
		)
		
		# Delete the return data
		pipeline.delete(
			"FAN_IN_RESULTS_" + fan_out_id
		)
		
		returned_data = pipeline.execute()
		
		# Raise an exception if the GET fails
		# This is how we achieve idempotency
		if returned_data[ 0 ] == None:
			raise AlreadyInvokedException()
			
		# Decode returned data
		decoded_return_data = []
		
		for returned_data_segment in returned_data[0]:
			# Try to JSON-decode each segment
			try:
				decoded_return_data.append(
					json.loads(
						returned_data_segment
					)
				)
			except:
				decoded_return_data.append(
					returned_data_segment
				)
		
		return decoded_return_data
	
	def _fan_in_operations( self, fan_out_id, return_data, **kwargs ):
		"""
		This function either returns False if we're not the last Lambda
		in the fan-in to be invoked or it returns a key of the data to be
		passed to the next function after the fan-in as input.
		"""
		if not self.redis_client:
			self.connect()
		
		# Do a redis pipeline transaction
		pipeline = self.redis_client.pipeline()
		
		# Push return data into redis
		pipeline.lpush(
			"FAN_IN_RESULTS_" + fan_out_id,
			json.dumps(
				return_data
			)
		)
		
		# Update expiration
		pipeline.expire(
			"FAN_IN_RESULTS_" + fan_out_id,
			( 60 * 15 * 3 ), # TODO dynamically calculate based off of nodes in path worst-case
		)
		
		# Decrement fan-in counter
		pipeline.decr(
			"FAN_IN_COUNTER_" + fan_out_id
		)
		
		# Get fan-in counter latest value
		pipeline.get(
			"FAN_IN_COUNTER_" + fan_out_id
		)
		
		returned_data = pipeline.execute()
		
		# Check fan-in counter value
		# If it's zero, we must move to invoke the next function
		fan_in_counter_result = int( returned_data[3] )
		
		if fan_in_counter_result == 0:
			# Clean up previous data
			pipeline = self.redis_client.pipeline()
			
			# Delete fan-in counter
			pipeline.delete(
				"FAN_IN_COUNTER_" + fan_out_id
			)
			
			returned_data = pipeline.execute()
			
			return "FAN_IN_RESULTS_" + fan_out_id
		
		return False
		
def _parallel_invoke( invoke_queue ):
	# TODO potential speed up by using a redis transaction to store all Lambda input data
	# in one pipelined action instead of the current request/response model.
	
	# List of active threads
	active_threads = []
	
	# Maximum number of threads
	max_threads = 50
	
	# Invokes a Lambda asynchronously
	def lambda_invoker_worker( execution_id, fan_out_ids, arn, input_data ):
		storage_key = gmemory._store_return_data_to_redis( input_data )
		
		return_wrapper_data = {
			"_refinery": {
				"indirect": {
					"type": "redis",
					"key": storage_key,
				},
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}
		
		lambda_client = boto3.client(
			"lambda"
		)
	
		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)
		
		raise SystemExit
		
	# Invokes a Lambda asynchronously with fan-in results as input
	def lambda_invoker_worker_fan_in( execution_id, fan_out_ids, arn, fan_out_id ):
		return_wrapper_data = {
			"_refinery": {
				"indirect": {
					"type": "redis_fan_in",
					"key": fan_out_id,
				},
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
			}
		}
		
		lambda_client = boto3.client(
			"lambda"
		)
	
		response = lambda_client.invoke(
			FunctionName=arn,
			InvocationType="Event",
			LogType="None",
			Payload=json.dumps(
				return_wrapper_data
			)
		)
		
		raise SystemExit
		
	# Publishes to an SNS topic
	def sns_topic_publish_worker( execution_id, fan_out_ids, arn, input_data ):
		# Handle large data that won't fit in the 256K SNS max size
		sns_client = boto3.client(
			"sns"
		)
		
		return_wrapper_data = {
			"_refinery": {
				"indirect": False,
				"parallel": False,
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"input_data": input_data,
			}
		}
	
		response = sns_client.publish(
			TopicArn=arn,
			Message=json.dumps(
				return_wrapper_data
			)
		)
		
		raise SystemExit
		
	# Publishes to redis to be picked up by a polling API Gateway Lambda
	def api_gateway_response( execution_id, arn, input_data ):
		# Store with expiration in redis
		gmemory.redis_client.setex(
			execution_id,
			gmemory.return_data_timeout,
			json.dumps(
				input_data
			)
		)
		
		raise SystemExit
	
	# Spawns a new worked and adds it to active_threads
	def spawn_new_worker():
		new_invoke_data = invoke_queue.pop()
		
		# Check type and invoke appropriately
		if new_invoke_data[ "type" ] == "lambda":
			new_thread = threading.Thread(
				target=lambda_invoker_worker,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "lambda_fan_in":
			new_thread = threading.Thread(
				target=lambda_invoker_worker_fan_in,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "fan_out_id" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "sns_topic":
			new_thread = threading.Thread(
				target=sns_topic_publish_worker,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "fan_out_ids" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()
			time.sleep(0.05) # Annoying wedge due to boto3 bug
		elif new_invoke_data[ "type" ] == "api_gateway_response":
			new_thread = threading.Thread(
				target=api_gateway_response,
				args=(
					new_invoke_data[ "execution_id" ],
					new_invoke_data[ "arn" ],
					new_invoke_data[ "input_data" ]
				)
			)
			new_thread.start()

		return new_thread
		
	# Bug fix test
	lambda_client = boto3.client(
		"lambda"
	)
	
	# Keep looping while there's still Lambdas to invoke
	while len( invoke_queue ) > 0:
		# If we've not maxing out acive threads then spawn new ones
		if len( active_threads ) < max_threads:
			active_threads.append(
				spawn_new_worker()
			)
		else:
			# Iterate over our active threads and remove finished
			new_active_thread_list = []
			already_spawned = False
			# Check if any threads are finished
			for active_thread in active_threads:
				if active_thread.is_alive():
					new_active_thread_list.append( active_thread )
				elif already_spawned == False:
					new_active_thread_list.append(
						spawn_new_worker()
					)
					already_spawned = True
					
			active_threads = new_active_thread_list
			
	# Wait until all threads finish
	for thread in active_threads:
		thread.join()
	
	# We're all done
	return

def _api_endpoint( execution_id, context ):
    # This will spin and continually query redis until either
    # we time out without getting our HTTP response OR we return
    # our response to the client.
    
    # Continually loop until we have only two seconds left
    # Max execution time is 30 seconds, so that's 28 seconds
    timed_out = True

    # As long as we have ~2 seconds of runway left continue
    # to query redis for our HTTP response data.
    while context.get_remaining_time_in_millis() > ( 2 * 1000 ):
    	try:
	        http_response = gmemory._get_input_data_from_redis(
	            execution_id
	        )
		except AlreadyInvokedException as e:
			print( "This Lambda has already been invoked (or the return data has expired). For this reason we are quitting out." )
			return
        
        # When we have a non-None http_response we can
        # break out of the loop and declare we've not timed out.
        if http_response:
            timed_out = False
            break
        
        # Wait a moment before checking again
        time.sleep( 0.01 )
    
    # We've timed out, return an error
    if timed_out:
        return {
        	"statusCode": 504,
        	"headers": {},
        	"body": json.dumps({
        	    "msg": "The request to the backend has timed out.",
        	    "success": False
        	}),
        	"isBase64Encoded": False
        }
    
    # Check if the response is actually in an API Gateway already
    # If not return as just regular JSON, if it is then return raw
    if "body" in http_response:
        return http_response
    
    # Return JSON response with the data
    return {
    	"statusCode": 200,
    	"headers": {
    		"Content-Type": "application/json",
    		"X-Frame-Options": "deny",
    		"X-Content-Type-Options": "nosniff",
    		"X-XSS-Protection": "1; mode=block",
    		"Cache-Control": "no-cache, no-store, must-revalidate",
    		"Pragma": "no-cache",
    		"Expires": "0",
    		"Server": "refinery"
    	},
    	"body": json.dumps( http_response ),
    	"isBase64Encoded": False
    }

def _init( lambda_input, context ):
	# TODO add timer
	# Bake in AWS region
	aws_region = "{{AWS_REGION_REPLACE_ME}}"
	
	# Indicate a special execution condition
	# For example, an API Gateway Lambda
	execution_mode = "{{SPECIAL_EXECUTION_MODE}}"
	
	# Construct log details for debugging
	log_details = {
		"initialization_time": int( time.time() ),
		"aws_region": aws_region,
		"group_name": context.log_group_name,
		"stream_name": context.log_stream_name,
		"function_name": context.function_name,
		"function_version": context.function_version,
		"invoked_function_arn": context.invoked_function_arn,
		"memory_limit_in_mb": context.memory_limit_in_mb,
		"aws_request_id": context.aws_request_id
	}
	
	global cmemory
	global gmemory
	
	cmemory = Refinery_Memory(
		"config-memory.refinery.thehackerblog.com",
		"{{REDIS_PASSWORD_REPLACE_ME}}",
		False
	)
	
	gmemory = Refinery_Memory(
		"global-memory.refinery.thehackerblog.com",
		"{{REDIS_PASSWORD_REPLACE_ME}}",
		context.function_name
	)
	
	# Execution ID, this is an ID which correlates to an execution chain
	execution_id = False
	
	# Bake in transition data
	transitions = json.loads( "{{TRANSITION_DATA_REPLACE_ME}}" )
	
	# Set default fan-out IDs list to be empty
	fan_out_ids = []
	
	# Detect refinery wrapper and unwrap if existant
	# Else just leave it unmodified
	if lambda_input and "_refinery" in lambda_input:
		# Set execution ID if set
		if "execution_id" in lambda_input[ "_refinery" ]:
			execution_id = lambda_input[ "_refinery" ][ "execution_id" ]
			
		# Set/propogate fan-out ID list if set
		if "fan_out_ids" in lambda_input[ "_refinery" ]:
			fan_out_ids = lambda_input[ "_refinery" ][ "fan_out_ids" ]
			
		_side_loaded = False
		if "indirect" in lambda_input[ "_refinery" ] and "type" in lambda_input[ "_refinery" ][ "indirect" ]:
			# Input data is stored in redis
			if lambda_input[ "_refinery" ][ "indirect" ][ "type" ] == "redis":
				try:
					lambda_input = gmemory._get_input_data_from_redis(
						lambda_input[ "_refinery" ][ "indirect" ][ "key" ]
					)
				except AlreadyInvokedException as e:
					print( "This Lambda has already been invoked (or the return data has expired). For this reason we are quitting out." )
					return
				_side_loaded = True
			# Input data is stored in redis as a list from a fan-in
			elif lambda_input[ "_refinery" ][ "indirect" ][ "type" ] == "redis_fan_in":
				try:
					lambda_input = gmemory._fan_in_get_results_data(
						lambda_input[ "_refinery" ][ "indirect" ][ "key" ]
					)
				except AlreadyInvokedException as e:
					print( "This Lambda has already been invoked (or the return data has expired). For this reason we are quitting out." )
					return
				_side_loaded = True
		
		if not _side_loaded and "input_data" in lambda_input[ "_refinery" ]:
			lambda_input = lambda_input[ "_refinery" ][ "input_data" ]
	
	# If we don't have an execution ID, generate and set one!
	if not execution_id:
		execution_id = str( uuid.uuid4() )
		
	# Set execution ID on Lambda context
	# This is needed so it can be accessed inside of the Lambdas
	# regular code.
	setattr(
		context,
		"execution_id",
		execution_id
	)
	
	return_data = {}
	
	if execution_mode == "REGULAR":
		# Catch any exception that occurs and handle it if a catch is defined.
		try:
			return_data = main( lambda_input, context )
		except:
			exception_string = str( traceback.format_exc() )
			
			# Invoke all Lambdas to be run when an exception occurs
			if len( transitions[ "exception" ] ) > 0:
				invocation_input_list = []
				for exception_transition_data in transitions[ "exception" ]:
					invocation_input_list.append({
						"execution_id": execution_id,
						"fan_out_ids": fan_out_ids,
						"type": exception_transition_data[ "type" ],
						"arn": exception_transition_data[ "arn" ],
						"input_data": {
							"version": _VERSION,
							"exception_text": exception_string,
							"input_data": json.loads(
								json.dumps(
									lambda_input
								)
							)
						}
					})
				
				# If there's an exception we should stop after invoking the exception case(s).
				_parallel_invoke( invocation_input_list )
				raise
			else:
				# Nothing to handle it, throw it again.
				raise
	
	# Stores the invocation data for "if"s and "then"s
	invocation_input_list = []
	
	# For fan-outs
	for fan_out_transition_data in transitions[ "fan-out" ]:
		if type( return_data ) != list:
			raise Exception( "Error, tried to fan-out without returning a list, returned type " + str( type( return_data ) ) )
		
		# Generate fan-out ID
		fan_out_id = str( uuid.uuid4() )
		
		# Add to our fan-out IDs
		new_fan_out_ids = fan_out_ids
		new_fan_out_ids.append(
			fan_out_id
		)
		
		# Set fan-in data in case we fan in later
		gmemory._set_fan_in_data(
			fan_out_id,
			len( return_data )
		)
		
		# Iterate over each item in the return list
		for return_item_data in return_data:
			# Create an invocation for it
			invocation_input_list.append({
				"execution_id": execution_id,
				"fan_out_ids": new_fan_out_ids,
				"type": fan_out_transition_data[ "type" ],
				"arn": fan_out_transition_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_item_data
					)
				)
			})
			
	# For fan-in
	for fan_in_transition_data in transitions[ "fan-in" ]:
		# Get last fan-out ID
		fan_out_id = fan_out_ids.pop()
		
		# First we push our returned data into the results list
		fan_in_results = gmemory._fan_in_operations(
			fan_out_id,
			return_data
		)
		
		# If we're the final invocation in the fan-in we
		# will invoke the next Lambda with the return data
		# array as the input.
		if fan_in_results:
			invocation_input_list.append({
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": "lambda_fan_in",
				"arn": fan_in_transition_data[ "arn" ],
				"fan_out_id": fan_out_id
			})
	
	# If it's just a then, just invoke the next Lambda
	for then_transition_data in transitions[ "then" ]:
		invocation_input_list.append({
			"execution_id": execution_id,
			"fan_out_ids": fan_out_ids,
			"type": then_transition_data[ "type" ],
			"arn": then_transition_data[ "arn" ],
			"input_data": json.loads(
				json.dumps(
					return_data
				)
			)
		})
	
	# If it's an API gateway Lambda we can end it here.
	if execution_mode == "API_ENDPOINT":
		# Now we invoke all the queued Lambdas!
		_parallel_invoke( invocation_input_list )
		
		return _api_endpoint( execution_id, context )
	
	# Variable to hold if any "if" statements evaluated to true
	# if one has then we don't execute any "else" statements
	true_if_evaluation_occured = False
	
	# Iterate over every if
	for if_statement_data in transitions[ "if" ]:
		expression_eval_result = eval( if_statement_data[ "expression" ] )
		
		if expression_eval_result:
			invocation_input_list.append({
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": if_statement_data[ "type" ],
				"arn": if_statement_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})
			
			true_if_evaluation_occured = True
		
	# If else is set, call that now
	if true_if_evaluation_occured == False:
		for else_transition_data in transitions[ "else" ]:
			invocation_input_list.append({
				"execution_id": execution_id,
				"fan_out_ids": fan_out_ids,
				"type": else_transition_data[ "type" ],
				"arn": else_transition_data[ "arn" ],
				"input_data": json.loads(
					json.dumps(
						return_data
					)
				)
			})
	
	# Now we invoke all the queued Lambdas!
	_parallel_invoke( invocation_input_list )
	
	# Return the return data as usual
	return return_data
